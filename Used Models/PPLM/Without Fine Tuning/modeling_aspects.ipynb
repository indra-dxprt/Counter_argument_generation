{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1957,"status":"ok","timestamp":1663610344931,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"Wsc98Qw3sJRz"},"outputs":[],"source":["!cp /content/drive/MyDrive/kialo_corpus.json ."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28687,"status":"ok","timestamp":1663610375040,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"BWEtlnYlsdY6","outputId":"49b8d893-208a-46ea-8d60-9ca41a8ea13f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycld2\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[K     |████████████████████████████████| 41.4 MB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: six\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy\u003e=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: numpy\u003e=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: smart-open\u003e=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: typer\u003c0.5.0,\u003e=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: thinc\u003c8.2.0,\u003e=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n","Requirement already satisfied: typing-extensions\u003c4.2.0,\u003e=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: pathy\u003e=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c1.10.0,\u003e=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c2.1.0,\u003e=2.0.6-\u003espacy) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003espacy) (3.0.9)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (2022.6.15)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (2.10)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc\u003c8.2.0,\u003e=8.1.0-\u003espacy) (0.7.8)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003espacy) (2.0.1)\n","Building wheels for collected packages: pycld2\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834217 sha256=4021f0cc0f974d1e6ba8860357837aadc1e3715179e305dcc06b55f22be8e507\n","  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n","Successfully built pycld2\n","Installing collected packages: pycld2\n","Successfully installed pycld2-0.41\n"]}],"source":["!pip install pycld2 regex nltk gensim spacy"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11820,"status":"ok","timestamp":1663610386852,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"5RWoVY6UsHgh","outputId":"384fcc5b-e680-4ab8-8b54-e4421efce4b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["5% of sentences done\n","10% of sentences done\n","15% of sentences done\n","20% of sentences done\n","25% of sentences done\n","30% of sentences done\n","35% of sentences done\n","40% of sentences done\n","45% of sentences done\n","50% of sentences done\n","55% of sentences done\n","60% of sentences done\n","65% of sentences done\n","70% of sentences done\n","75% of sentences done\n","80% of sentences done\n","85% of sentences done\n","90% of sentences done\n","95% of sentences done\n","100% of sentences done\n"]}],"source":["import json\n","import pycld2 as cld2\n","\n","with open('kialo_corpus.json', 'r') as f:\n","    out = json.load(f)\n","\n","# Need to use regex to remove offending non-UTF-8 characters from the data, which\n","# causes (error: input contains invalid UTF-8 around byte ...)\n","# Ref: https://github.com/aboSamoor/polyglot/issues/71#issuecomment-707997790\n","\n","import regex\n","import math\n","\n","RE_BAD_CHARS = regex.compile(r\"[\\p{Cc}\\p{Cs}]+\")\n","\n","def remove_bad_chars(text):\n","    return RE_BAD_CHARS.sub(\"\", text)\n","\n","def detect_en(text):\n","    _, _, _, detection = cld2.detect(text, returnVectors=True)\n","    for tup in detection:\n","        if 'en' not in tup[-1]:\n","            return False\n","    return True\n","\n","out_filtered = []\n","prev_num = 0\n","for i in range(len(out)):\n","    x = out[i]\n","    try:\n","        x['text'] = remove_bad_chars(x['text'])\n","        if detect_en(x['text']):\n","            out_filtered.append(x)\n","    except Exception as e:\n","        print(x['text'])\n","        print(f'Exception {e} raised')\n","        break\n","    percent = (i+1) / len(out) * 100\n","    _, num = math.modf(percent)\n","    num_ = int(num - (num % 5))\n","    if num_ != prev_num:\n","        for x in range(prev_num+5, num_+1, 5):\n","            print(f\"{x}% of sentences done\")\n","        prev_num = num_"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2959,"status":"ok","timestamp":1663610390194,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"AjG8_f4Ir5SR","outputId":"dd8dcb38-5898-468c-95dc-fc87cc01e615"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["import nltk\n","from nltk.stem import SnowballStemmer, WordNetLemmatizer\n","import re\n","import os\n","import pickle\n","import gensim\n","from tqdm import tqdm\n","\n","nltk.download(\"wordnet\")\n","nltk.download(\"omw-1.4\")\n","\n","stemmer = SnowballStemmer(\"english\")\n","\n","models = {}\n","word_map = {}\n","debug = False\n","drive_path = '/content/drive/MyDrive'\n","num_topics = 50\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","# Tokenize and lemmatize\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text) :\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) \u003e 3:\n","            result.append(lemmatize_stemming(token))\n","            \n","    return result\n","\n","def sentence_to_seq(text):\n","    split_words = set(text.split())\n","    tokens = preprocess(re.sub(r'http\\S+', '', text))\n","\n","    # Keep a mapping of stems to original words\n","    if not os.path.exists('word_map.pkl'):\n","        for tk in tokens:\n","            for word in split_words:\n","                if tk in word:\n","                    if not word_map.get(tk):\n","                        word_map[tk] = set()\n","                    word_map[tk].add(word)\n","    \n","    return tokens\n","\n","def prep_docs(out_filtered):\n","  all_docs = []\n","  all_sents = []\n","  for i, x in enumerate(tqdm(out_filtered, ascii=True)):\n","    topic_id, _ = x['id'].strip().split('.')\n","    if not models.get(topic_id):\n","      models[topic_id] = {}\n","    if x['neutral']:\n","      all_sents.append(x['neutral'][0]['text'])\n","      seq = sentence_to_seq(x['neutral'][0]['text'])\n","      models[topic_id]['topic'] = seq\n","      all_docs.append(seq)\n","\n","    for obj in x['pro']:\n","      if not models[topic_id].get('pro'):\n","        models[topic_id]['pro'] = []\n","      all_sents.append(obj['text'])\n","      seq = sentence_to_seq(obj['text'])\n","      models[topic_id]['pro'].append(seq)\n","      all_docs.append(seq)\n","    \n","    for obj in x['con']:\n","      if not models[topic_id].get('con'):\n","        models[topic_id]['con'] = []\n","      all_sents.append(obj['text'])\n","      seq = sentence_to_seq(obj['text'])\n","      models[topic_id]['con'].append(seq)\n","      all_docs.append(seq)\n","\n","  return all_docs, all_sents\n","\n","def model_topics(processed_docs, num_topics=10):\n","    os.makedirs(os.path.join(drive_path, 'kialo_topics'), exist_ok=True)\n","    model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","    if os.path.exists(model_path):\n","        lda_model = gensim.models.LdaMulticore.load(model_path)\n","    else:\n","        dictionary = gensim.corpora.Dictionary(processed_docs)\n","        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","        lda_model =  gensim.models.LdaMulticore(\n","            bow_corpus, num_topics = num_topics,\n","            id2word = dictionary, passes = 10, workers = 8\n","        )\n","        # Save the model\n","        lda_model.save(model_path)\n","\n","    # Print each topic discovered with it's top-40 words (tokens)\n","    # for idx, topic in lda_model.print_topics(num_words=40):\n","    #    topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","    #    topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","    #    topic_words = [min(x, key=len) if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)]\n","    #    if debug:\n","    #        print(\"Topic: {} \\nWords: {}\".format(idx, ','.join(topic_words)))\n","    #        print(\"\\n\")\n","\n","def model_subtopics(processed_docs, topic_id, stance):\n","    fname = f'models/lda_topics_{topic_id}_{stance}.ckpt'\n","    if os.path.exists(fname):\n","        lda_model = gensim.models.LdaMulticore.load(fname)\n","    else:\n","        dictionary = gensim.corpora.Dictionary(processed_docs)\n","        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","        lda_model =  gensim.models.LdaMulticore(\n","            bow_corpus, num_topics = 10, id2word = dictionary, passes = 20, workers = 4\n","        )\n","        # Save the model\n","        lda_model.save(fname)\n","    \n","    if debug:\n","      # Print each topic discovered with it's top-40 words (tokens)\n","      for idx, topic in lda_model.print_topics(num_words=5):\n","          topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","          topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","          topic_words = [min(x, key=len) if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)]\n","          print(f\"For topic ID {topic_id} and stance {stance}\")\n","          print(\"Topic: {} =\u003e Words: {}\".format(idx, ','.join(topic_words)))\n","          #fpath = os.path.join('topics', f'topic{idx+1}.txt')\n","          #with open(fpath, 'w') as f:\n","          #    for w in topic_words[:-1]:\n","          #        f.write(f'{w}\\n')\n","          #    f.write(f'{topic_words[-1]}')\n","    \n","    return fname\n","\n","def run_topic_modeling():\n","  global models, word_map, debug, num_topics\n","  all_docs = []\n","  all_sents = []\n","  \"\"\"\n","  if not os.path.exists(os.path.join(drive_path, 'topic_models.json')):\n","    prep_docs(out_filtered)\n","    os.makedirs(os.path.join(drive_path, 'models'), exist_ok=True)\n","    for _, (k, v) in enumerate(tqdm(models.items(), ascii=True)):\n","      models[k]['pro_save_name'] = model_subtopics(v['pro'], k, 'pro') if v.get('pro') else None\n","      models[k]['con_save_name'] = model_subtopics(v['con'], k, 'con') if v.get('con') else None\n","  else:\n","    with open(os.path.join(drive_path, 'topic_models.json'), 'r') as f:\n","      models = json.load(f)\n","  \"\"\"\n","  topic_model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","  if not os.path.exists(topic_model_path):\n","    all_docs, all_sents = prep_docs(out_filtered)\n","    model_topics(all_docs, num_topics=num_topics)\n","  else:\n","    pass\n","    #all_docs, all_sents = prep_docs(out_filtered)\n","\n","  if not os.path.exists(os.path.join(drive_path, 'word_map.pkl')):\n","    with open(os.path.join(drive_path, 'word_map.pkl'), 'wb') as f:\n","      pickle.dump(word_map, f)\n","  else:\n","    with open(os.path.join(drive_path, 'word_map.pkl'), 'rb') as f:\n","      word_map = pickle.load(f)\n","\n","  return all_docs, all_sents\n","\n","all_docs, all_sents = run_topic_modeling()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14050,"status":"ok","timestamp":1663610404628,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"eZDO0XsBQs9s","outputId":"075136ce-135b-4341-950f-b95a603665a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-09-19 17:59:57.228911: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: spacy\u003c3.5.0,\u003e=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c1.10.0,\u003e=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: typing-extensions\u003c4.2.0,\u003e=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: pathy\u003e=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: thinc\u003c8.2.0,\u003e=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: typer\u003c0.5.0,\u003e=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c2.1.0,\u003e=2.0.6-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open\u003c6.0.0,\u003e=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy\u003e=0.3.5-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc\u003c8.2.0,\u003e=8.1.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click\u003c9.0.0,\u003e=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer\u003c0.5.0,\u003e=0.3.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003een-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5610,"status":"ok","timestamp":1663610410227,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"D_kPMsJ6SFnp","outputId":"294380fb-38e3-42f6-a8cf-e794cc9c26b4"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["Topic 0: 0.094*\"power\" + 0.037*\"intern\" + 0.031*\"countri\" + 0.026*\"ban\" + 0.025*\"illeg\" + 0.024*\"control\" + 0.022*\"cooper\" + 0.022*\"china\" + 0.021*\"threat\" + 0.019*\"conflict\" + 0.018*\"measur\" + 0.014*\"peac\" + 0.013*\"border\" + 0.013*\"check\" + 0.013*\"sign\" + 0.012*\"israel\" + 0.012*\"russia\" + 0.011*\"balanc\" + 0.010*\"govern\" + 0.009*\"turkey\" + 0.008*\"aggress\" + 0.008*\"regim\" + 0.007*\"bind\" + 0.006*\"influenc\" + 0.006*\"weak\" + 0.006*\"like\" + 0.006*\"blame\" + 0.006*\"council\" + 0.006*\"world\" + 0.006*\"trade\" + 0.005*\"enforc\" + 0.005*\"extern\" + 0.005*\"palestinian\" + 0.005*\"territori\" + 0.005*\"secur\" + 0.005*\"treati\" + 0.005*\"hostil\" + 0.005*\"allow\" + 0.005*\"order\" + 0.004*\"prevent\"\n","Topic 1: 0.057*\"space\" + 0.047*\"earth\" + 0.033*\"event\" + 0.024*\"destroy\" + 0.023*\"domest\" + 0.021*\"planet\" + 0.021*\"travel\" + 0.019*\"light\" + 0.018*\"time\" + 0.016*\"speci\" + 0.014*\"extinct\" + 0.013*\"convent\" + 0.011*\"carbon\" + 0.011*\"speed\" + 0.010*\"entertain\" + 0.010*\"explor\" + 0.010*\"caus\" + 0.008*\"endang\" + 0.008*\"mission\" + 0.008*\"cycl\" + 0.008*\"west\" + 0.007*\"habit\" + 0.007*\"distanc\" + 0.007*\"starv\" + 0.006*\"detect\" + 0.006*\"atmospher\" + 0.006*\"zone\" + 0.006*\"symptom\" + 0.006*\"emiss\" + 0.006*\"like\" + 0.006*\"surfac\" + 0.006*\"ocean\" + 0.005*\"harri\" + 0.005*\"passeng\" + 0.005*\"ring\" + 0.005*\"temperatur\" + 0.005*\"bike\" + 0.005*\"casualti\" + 0.005*\"star\" + 0.004*\"battl\"\n","Topic 2: 0.072*\"educ\" + 0.060*\"school\" + 0.051*\"student\" + 0.036*\"learn\" + 0.026*\"gun\" + 0.026*\"teacher\" + 0.026*\"teach\" + 0.022*\"shoot\" + 0.021*\"train\" + 0.019*\"mass\" + 0.018*\"skill\" + 0.017*\"degre\" + 0.013*\"colleg\" + 0.011*\"like\" + 0.010*\"field\" + 0.010*\"need\" + 0.009*\"requir\" + 0.009*\"cours\" + 0.008*\"background\" + 0.008*\"kid\" + 0.008*\"univers\" + 0.007*\"class\" + 0.007*\"high\" + 0.007*\"academ\" + 0.007*\"attend\" + 0.007*\"better\" + 0.007*\"prepar\" + 0.007*\"time\" + 0.007*\"access\" + 0.007*\"public\" + 0.006*\"allow\" + 0.006*\"provid\" + 0.006*\"level\" + 0.005*\"studi\" + 0.005*\"qualifi\" + 0.005*\"experi\" + 0.005*\"subject\" + 0.005*\"environ\" + 0.004*\"program\" + 0.004*\"color\"\n","Topic 3: 0.105*\"good\" + 0.060*\"free\" + 0.052*\"evil\" + 0.045*\"choic\" + 0.036*\"thing\" + 0.027*\"action\" + 0.027*\"choos\" + 0.023*\"exist\" + 0.019*\"reason\" + 0.016*\"perfect\" + 0.016*\"world\" + 0.014*\"possibl\" + 0.012*\"mean\" + 0.012*\"justifi\" + 0.011*\"creat\" + 0.010*\"person\" + 0.009*\"necessarili\" + 0.008*\"greater\" + 0.008*\"allow\" + 0.008*\"requir\" + 0.007*\"permit\" + 0.007*\"necessari\" + 0.007*\"absolut\" + 0.007*\"determin\" + 0.007*\"maxim\" + 0.007*\"justif\" + 0.006*\"moral\" + 0.006*\"univers\" + 0.005*\"best\" + 0.005*\"mind\" + 0.005*\"give\" + 0.005*\"unnecessari\" + 0.005*\"kant\" + 0.005*\"virtu\" + 0.005*\"categor\" + 0.005*\"suffer\" + 0.005*\"imper\" + 0.005*\"exampl\" + 0.005*\"need\" + 0.005*\"agent\"\n","Topic 4: 0.062*\"crime\" + 0.038*\"crimin\" + 0.037*\"commit\" + 0.034*\"punish\" + 0.031*\"murder\" + 0.025*\"victim\" + 0.021*\"justic\" + 0.019*\"fear\" + 0.016*\"judg\" + 0.016*\"suicid\" + 0.013*\"rape\" + 0.012*\"case\" + 0.012*\"bullfight\" + 0.011*\"convict\" + 0.010*\"act\" + 0.010*\"like\" + 0.010*\"sever\" + 0.009*\"violenc\" + 0.009*\"death\" + 0.008*\"innoc\" + 0.008*\"kill\" + 0.008*\"homicid\" + 0.008*\"mistak\" + 0.008*\"uneth\" + 0.008*\"gun\" + 0.007*\"sentenc\" + 0.007*\"violent\" + 0.007*\"person\" + 0.007*\"steal\" + 0.007*\"worri\" + 0.006*\"bull\" + 0.006*\"arrest\" + 0.006*\"abus\" + 0.006*\"action\" + 0.005*\"prevent\" + 0.005*\"perpetr\" + 0.005*\"judgement\" + 0.005*\"result\" + 0.005*\"exampl\" + 0.004*\"respons\"\n","Topic 5: 0.118*\"social\" + 0.069*\"gender\" + 0.033*\"role\" + 0.030*\"ident\" + 0.028*\"game\" + 0.025*\"play\" + 0.024*\"bias\" + 0.022*\"construct\" + 0.019*\"biolog\" + 0.018*\"structur\" + 0.017*\"program\" + 0.012*\"perceiv\" + 0.012*\"stereotyp\" + 0.012*\"percept\" + 0.011*\"topic\" + 0.011*\"discuss\" + 0.011*\"alien\" + 0.010*\"societ\" + 0.010*\"attitud\" + 0.009*\"video\" + 0.009*\"media\" + 0.008*\"exclus\" + 0.008*\"neutral\" + 0.007*\"reinforc\" + 0.007*\"base\" + 0.007*\"develop\" + 0.007*\"softwar\" + 0.006*\"perpetu\" + 0.006*\"sensit\" + 0.006*\"creat\" + 0.006*\"autonom\" + 0.006*\"import\" + 0.006*\"form\" + 0.006*\"norm\" + 0.005*\"influenc\" + 0.005*\"interact\" + 0.005*\"behavior\" + 0.005*\"mutual\" + 0.004*\"person\" + 0.004*\"model\"\n","Topic 6: 0.075*\"increas\" + 0.045*\"econom\" + 0.034*\"countri\" + 0.030*\"incom\" + 0.029*\"higher\" + 0.025*\"economi\" + 0.024*\"benefit\" + 0.022*\"high\" + 0.018*\"rat\" + 0.018*\"lower\" + 0.016*\"wealth\" + 0.016*\"rate\" + 0.015*\"decreas\" + 0.015*\"poverti\" + 0.015*\"contribut\" + 0.014*\"level\" + 0.014*\"develop\" + 0.013*\"number\" + 0.012*\"inequ\" + 0.012*\"capit\" + 0.010*\"growth\" + 0.010*\"lead\" + 0.010*\"invest\" + 0.010*\"reduc\" + 0.009*\"signific\" + 0.009*\"trade\" + 0.009*\"overal\" + 0.008*\"rich\" + 0.008*\"unemploy\" + 0.008*\"currenc\" + 0.008*\"purchas\" + 0.007*\"factor\" + 0.007*\"wag\" + 0.006*\"poor\" + 0.006*\"declin\" + 0.006*\"result\" + 0.005*\"inflat\" + 0.005*\"greater\" + 0.005*\"effect\" + 0.005*\"financi\"\n","Topic 7: 0.086*\"harm\" + 0.056*\"risk\" + 0.047*\"relationship\" + 0.043*\"kill\" + 0.037*\"save\" + 0.036*\"danger\" + 0.025*\"situat\" + 0.025*\"safeti\" + 0.021*\"safe\" + 0.019*\"doctor\" + 0.017*\"caus\" + 0.017*\"person\" + 0.016*\"potenti\" + 0.016*\"vaccin\" + 0.014*\"respect\" + 0.009*\"prevent\" + 0.009*\"watch\" + 0.009*\"scenario\" + 0.008*\"pose\" + 0.007*\"case\" + 0.007*\"digniti\" + 0.007*\"mitig\" + 0.006*\"trust\" + 0.006*\"cruel\" + 0.006*\"abandon\" + 0.006*\"threat\" + 0.006*\"avoid\" + 0.006*\"street\" + 0.005*\"interven\" + 0.005*\"predat\" + 0.005*\"stop\" + 0.005*\"creativ\" + 0.005*\"cheat\" + 0.005*\"partner\" + 0.005*\"pet\" + 0.005*\"condom\" + 0.004*\"doubl\" + 0.004*\"brothel\" + 0.004*\"reason\" + 0.004*\"routin\"\n","Topic 8: 0.164*\"societi\" + 0.156*\"individu\" + 0.055*\"communiti\" + 0.022*\"prison\" + 0.016*\"collect\" + 0.013*\"vulner\" + 0.013*\"lgbt\" + 0.012*\"expos\" + 0.012*\"exclud\" + 0.011*\"expert\" + 0.011*\"irrelev\" + 0.010*\"member\" + 0.009*\"lgbtq\" + 0.009*\"benefit\" + 0.008*\"compel\" + 0.007*\"accept\" + 0.006*\"isol\" + 0.006*\"contribut\" + 0.006*\"chariti\" + 0.006*\"help\" + 0.006*\"offend\" + 0.005*\"provid\" + 0.005*\"neglect\" + 0.005*\"binari\" + 0.005*\"person\" + 0.005*\"allow\" + 0.004*\"respons\" + 0.004*\"rehabilit\" + 0.004*\"unaccept\" + 0.004*\"abstin\" + 0.004*\"unfound\" + 0.004*\"teenag\" + 0.004*\"euthanasia\" + 0.004*\"societ\" + 0.004*\"base\" + 0.004*\"sentenc\" + 0.004*\"posit\" + 0.003*\"imprison\" + 0.003*\"general\" + 0.003*\"give\"\n","Topic 9: 0.149*\"chang\" + 0.100*\"sexual\" + 0.046*\"bodi\" + 0.039*\"desir\" + 0.035*\"male\" + 0.031*\"femal\" + 0.028*\"climat\" + 0.023*\"birth\" + 0.021*\"partner\" + 0.014*\"assault\" + 0.013*\"characterist\" + 0.012*\"harass\" + 0.011*\"nippl\" + 0.009*\"control\" + 0.009*\"orient\" + 0.009*\"satisfi\" + 0.008*\"worship\" + 0.008*\"person\" + 0.007*\"abus\" + 0.006*\"time\" + 0.006*\"domin\" + 0.006*\"physic\" + 0.005*\"accid\" + 0.005*\"injuri\" + 0.005*\"show\" + 0.005*\"caus\" + 0.005*\"undergo\" + 0.005*\"like\" + 0.005*\"woman\" + 0.004*\"need\" + 0.004*\"mean\" + 0.004*\"hasn\" + 0.004*\"masculin\" + 0.004*\"view\" + 0.004*\"behavior\" + 0.004*\"virgin\" + 0.004*\"practic\" + 0.004*\"consid\" + 0.004*\"happen\" + 0.003*\"relat\"\n","Topic 10: 0.163*\"women\" + 0.079*\"equal\" + 0.033*\"achiev\" + 0.030*\"opportun\" + 0.026*\"goal\" + 0.023*\"discrimin\" + 0.023*\"movement\" + 0.018*\"happi\" + 0.013*\"wear\" + 0.013*\"woman\" + 0.013*\"fulfil\" + 0.012*\"pursu\" + 0.012*\"privileg\" + 0.010*\"career\" + 0.010*\"gender\" + 0.010*\"cloth\" + 0.009*\"profess\" + 0.009*\"detriment\" + 0.009*\"femin\" + 0.008*\"feminist\" + 0.008*\"outcom\" + 0.007*\"empow\" + 0.007*\"mean\" + 0.007*\"strength\" + 0.006*\"posit\" + 0.006*\"give\" + 0.006*\"workplac\" + 0.006*\"promot\" + 0.005*\"allow\" + 0.005*\"succeed\" + 0.005*\"merit\" + 0.005*\"like\" + 0.005*\"pursuit\" + 0.005*\"fair\" + 0.005*\"need\" + 0.004*\"have\" + 0.004*\"sham\" + 0.004*\"face\" + 0.004*\"strive\" + 0.004*\"dress\"\n","Topic 11: 0.094*\"activ\" + 0.050*\"particip\" + 0.030*\"sport\" + 0.025*\"worth\" + 0.022*\"attract\" + 0.019*\"correl\" + 0.015*\"foreign\" + 0.015*\"ask\" + 0.013*\"barrier\" + 0.013*\"belong\" + 0.012*\"unclear\" + 0.012*\"engag\" + 0.012*\"competit\" + 0.011*\"item\" + 0.011*\"elder\" + 0.010*\"visit\" + 0.010*\"tourism\" + 0.009*\"leagu\" + 0.009*\"endors\" + 0.009*\"youth\" + 0.008*\"crucial\" + 0.008*\"conveni\" + 0.007*\"style\" + 0.007*\"like\" + 0.007*\"criteria\" + 0.007*\"fighter\" + 0.007*\"wider\" + 0.007*\"causat\" + 0.007*\"cannib\" + 0.007*\"compass\" + 0.006*\"suppress\" + 0.006*\"distinguish\" + 0.006*\"unwil\" + 0.006*\"entri\" + 0.006*\"dispar\" + 0.006*\"socio\" + 0.006*\"quran\" + 0.006*\"senior\" + 0.005*\"guidelin\" + 0.005*\"smart\"\n","Topic 12: 0.120*\"legal\" + 0.078*\"drug\" + 0.067*\"death\" + 0.044*\"purpos\" + 0.026*\"prostitut\" + 0.025*\"execut\" + 0.022*\"illeg\" + 0.022*\"demand\" + 0.021*\"normal\" + 0.018*\"addict\" + 0.017*\"legalis\" + 0.012*\"valuabl\" + 0.012*\"caus\" + 0.012*\"penalti\" + 0.011*\"divis\" + 0.010*\"substanc\" + 0.010*\"effect\" + 0.009*\"mandat\" + 0.009*\"deterr\" + 0.008*\"flaw\" + 0.008*\"legisl\" + 0.007*\"lethal\" + 0.007*\"creatur\" + 0.007*\"enhanc\" + 0.007*\"abolish\" + 0.007*\"anymor\" + 0.006*\"sweden\" + 0.005*\"partial\" + 0.005*\"lead\" + 0.005*\"pain\" + 0.005*\"realist\" + 0.005*\"like\" + 0.005*\"prohibit\" + 0.005*\"stage\" + 0.005*\"respons\" + 0.005*\"confin\" + 0.005*\"mean\" + 0.005*\"tobacco\" + 0.005*\"voluntarili\" + 0.004*\"capita\"\n","Topic 13: 0.176*\"right\" + 0.059*\"protect\" + 0.040*\"freedom\" + 0.029*\"constitut\" + 0.026*\"rule\" + 0.023*\"restrict\" + 0.021*\"violat\" + 0.020*\"court\" + 0.019*\"govern\" + 0.018*\"civil\" + 0.014*\"amend\" + 0.014*\"duti\" + 0.013*\"law\" + 0.012*\"principl\" + 0.011*\"person\" + 0.011*\"legal\" + 0.010*\"suprem\" + 0.010*\"grant\" + 0.009*\"fundament\" + 0.009*\"citizen\" + 0.008*\"declar\" + 0.008*\"limit\" + 0.008*\"liberti\" + 0.008*\"exercis\" + 0.007*\"deni\" + 0.007*\"defend\" + 0.007*\"case\" + 0.006*\"basic\" + 0.006*\"allow\" + 0.006*\"prosecut\" + 0.005*\"enforc\" + 0.005*\"respect\" + 0.005*\"persecut\" + 0.005*\"infring\" + 0.005*\"includ\" + 0.005*\"self\" + 0.005*\"guarante\" + 0.004*\"mean\" + 0.004*\"ownership\" + 0.004*\"privaci\"\n","Topic 14: 0.094*\"anim\" + 0.047*\"food\" + 0.034*\"meat\" + 0.028*\"produc\" + 0.026*\"environ\" + 0.021*\"genet\" + 0.021*\"human\" + 0.021*\"product\" + 0.019*\"eat\" + 0.017*\"consum\" + 0.016*\"grow\" + 0.015*\"farm\" + 0.015*\"water\" + 0.015*\"vegan\" + 0.015*\"plant\" + 0.012*\"consumpt\" + 0.012*\"healthi\" + 0.011*\"hunt\" + 0.011*\"diet\" + 0.011*\"zoo\" + 0.011*\"gorilla\" + 0.010*\"crop\" + 0.010*\"natur\" + 0.009*\"environment\" + 0.009*\"kill\" + 0.009*\"like\" + 0.008*\"agricultur\" + 0.007*\"need\" + 0.007*\"fee\" + 0.006*\"marijuana\" + 0.006*\"speci\" + 0.006*\"land\" + 0.005*\"vegetarian\" + 0.005*\"live\" + 0.005*\"livestock\" + 0.005*\"fruit\" + 0.005*\"caus\" + 0.004*\"suffer\" + 0.004*\"fish\" + 0.004*\"ethic\"\n","Topic 15: 0.078*\"organ\" + 0.026*\"special\" + 0.024*\"donat\" + 0.022*\"investig\" + 0.022*\"die\" + 0.022*\"wouldn\" + 0.018*\"truli\" + 0.018*\"symbol\" + 0.017*\"network\" + 0.017*\"code\" + 0.014*\"accus\" + 0.014*\"resist\" + 0.014*\"presenc\" + 0.013*\"modifi\" + 0.012*\"wikipedia\" + 0.012*\"press\" + 0.012*\"week\" + 0.010*\"fire\" + 0.010*\"catch\" + 0.010*\"time\" + 0.009*\"card\" + 0.009*\"guilti\" + 0.009*\"label\" + 0.009*\"hour\" + 0.009*\"penc\" + 0.009*\"guilt\" + 0.008*\"atom\" + 0.007*\"volunt\" + 0.007*\"communist\" + 0.007*\"case\" + 0.007*\"last\" + 0.007*\"flag\" + 0.007*\"charit\" + 0.006*\"notic\" + 0.006*\"doubt\" + 0.006*\"weigh\" + 0.006*\"variabl\" + 0.006*\"walk\" + 0.005*\"injur\" + 0.005*\"confess\"\n","Topic 16: 0.053*\"sourc\" + 0.035*\"drive\" + 0.034*\"build\" + 0.030*\"energi\" + 0.027*\"attack\" + 0.025*\"adopt\" + 0.023*\"citi\" + 0.017*\"wast\" + 0.017*\"car\" + 0.017*\"fuel\" + 0.014*\"transport\" + 0.013*\"power\" + 0.013*\"quick\" + 0.012*\"nuclear\" + 0.012*\"time\" + 0.010*\"vehicl\" + 0.010*\"electr\" + 0.010*\"charg\" + 0.009*\"effici\" + 0.009*\"road\" + 0.009*\"destruct\" + 0.008*\"york\" + 0.008*\"requir\" + 0.008*\"fossil\" + 0.008*\"materi\" + 0.008*\"store\" + 0.008*\"renew\" + 0.007*\"need\" + 0.007*\"bitcoin\" + 0.007*\"taxpay\" + 0.007*\"russian\" + 0.006*\"rough\" + 0.006*\"clean\" + 0.006*\"infrastructur\" + 0.006*\"wind\" + 0.006*\"solar\" + 0.006*\"self\" + 0.005*\"cover\" + 0.005*\"crowd\" + 0.005*\"switch\"\n","Topic 17: 0.041*\"user\" + 0.028*\"wide\" + 0.028*\"organis\" + 0.028*\"manag\" + 0.028*\"account\" + 0.023*\"platform\" + 0.022*\"rang\" + 0.020*\"advertis\" + 0.020*\"sure\" + 0.019*\"terrorist\" + 0.017*\"identifi\" + 0.016*\"terror\" + 0.013*\"bathroom\" + 0.013*\"block\" + 0.013*\"difficulti\" + 0.013*\"sit\" + 0.012*\"reproduct\" + 0.012*\"counter\" + 0.012*\"credit\" + 0.012*\"trial\" + 0.011*\"resolv\" + 0.011*\"privaci\" + 0.010*\"board\" + 0.010*\"drink\" + 0.009*\"calcul\" + 0.008*\"disput\" + 0.008*\"clinic\" + 0.008*\"promin\" + 0.008*\"encount\" + 0.008*\"uniform\" + 0.007*\"sake\" + 0.007*\"accommod\" + 0.006*\"uncertainti\" + 0.006*\"maximum\" + 0.006*\"unisex\" + 0.006*\"invas\" + 0.006*\"eas\" + 0.006*\"staff\" + 0.006*\"shut\" + 0.006*\"membership\"\n","Topic 18: 0.210*\"state\" + 0.076*\"nation\" + 0.048*\"citizen\" + 0.041*\"unit\" + 0.035*\"countri\" + 0.029*\"member\" + 0.023*\"immigr\" + 0.023*\"european\" + 0.023*\"govern\" + 0.016*\"local\" + 0.015*\"feder\" + 0.011*\"land\" + 0.010*\"union\" + 0.009*\"region\" + 0.009*\"world\" + 0.008*\"redistribut\" + 0.007*\"recognis\" + 0.007*\"perman\" + 0.007*\"congress\" + 0.007*\"own\" + 0.007*\"resid\" + 0.007*\"largest\" + 0.006*\"independ\" + 0.006*\"head\" + 0.006*\"strategi\" + 0.006*\"confeder\" + 0.005*\"level\" + 0.005*\"current\" + 0.005*\"foreign\" + 0.005*\"citizenship\" + 0.005*\"support\" + 0.005*\"affair\" + 0.004*\"institut\" + 0.004*\"rank\" + 0.004*\"commiss\" + 0.004*\"control\" + 0.004*\"reserv\" + 0.004*\"divid\" + 0.004*\"found\" + 0.004*\"prioriti\"\n","Topic 19: 0.069*\"vote\" + 0.041*\"democrat\" + 0.040*\"elect\" + 0.036*\"parti\" + 0.031*\"democraci\" + 0.030*\"voter\" + 0.029*\"repres\" + 0.021*\"referendum\" + 0.018*\"polit\" + 0.016*\"candid\" + 0.016*\"politician\" + 0.016*\"popular\" + 0.014*\"elector\" + 0.013*\"govern\" + 0.013*\"campaign\" + 0.011*\"brexit\" + 0.009*\"support\" + 0.009*\"leav\" + 0.009*\"public\" + 0.009*\"like\" + 0.008*\"major\" + 0.007*\"reform\" + 0.007*\"presid\" + 0.007*\"result\" + 0.007*\"issu\" + 0.007*\"parliament\" + 0.006*\"colleg\" + 0.006*\"general\" + 0.006*\"interest\" + 0.006*\"legisl\" + 0.005*\"moder\" + 0.005*\"senat\" + 0.005*\"citizen\" + 0.005*\"favour\" + 0.005*\"deal\" + 0.005*\"direct\" + 0.005*\"proport\" + 0.005*\"hold\" + 0.005*\"presidenti\" + 0.005*\"monarch\"\n","Topic 20: 0.063*\"real\" + 0.050*\"marriag\" + 0.044*\"break\" + 0.023*\"coupl\" + 0.022*\"rare\" + 0.021*\"actor\" + 0.019*\"ideal\" + 0.018*\"homosexu\" + 0.016*\"older\" + 0.015*\"movi\" + 0.013*\"star\" + 0.012*\"explicit\" + 0.012*\"divorc\" + 0.011*\"film\" + 0.011*\"secret\" + 0.010*\"charact\" + 0.009*\"mislead\" + 0.009*\"appar\" + 0.009*\"request\" + 0.008*\"seri\" + 0.008*\"arrang\" + 0.008*\"club\" + 0.007*\"procreat\" + 0.007*\"portray\" + 0.007*\"distract\" + 0.007*\"heterosexu\" + 0.007*\"asexu\" + 0.007*\"resort\" + 0.007*\"younger\" + 0.007*\"stimul\" + 0.007*\"healthier\" + 0.007*\"plausibl\" + 0.007*\"cast\" + 0.006*\"correspond\" + 0.006*\"show\" + 0.006*\"fiction\" + 0.006*\"luke\" + 0.006*\"scene\" + 0.006*\"exampl\" + 0.006*\"inabl\"\n","Topic 21: 0.085*\"inform\" + 0.034*\"data\" + 0.026*\"media\" + 0.021*\"damag\" + 0.019*\"violent\" + 0.019*\"brain\" + 0.017*\"public\" + 0.016*\"post\" + 0.015*\"imag\" + 0.015*\"poll\" + 0.014*\"drone\" + 0.014*\"accur\" + 0.011*\"news\" + 0.011*\"incid\" + 0.010*\"manipul\" + 0.010*\"approv\" + 0.009*\"cancer\" + 0.009*\"reveal\" + 0.009*\"alter\" + 0.008*\"show\" + 0.008*\"reput\" + 0.007*\"piec\" + 0.007*\"facebook\" + 0.007*\"hack\" + 0.007*\"paper\" + 0.007*\"confus\" + 0.006*\"sourc\" + 0.006*\"pictur\" + 0.006*\"donald\" + 0.006*\"coverag\" + 0.006*\"like\" + 0.006*\"figur\" + 0.006*\"ballot\" + 0.006*\"internship\" + 0.006*\"deliv\" + 0.006*\"journalist\" + 0.005*\"research\" + 0.005*\"easili\" + 0.005*\"tactic\" + 0.005*\"inaccur\"\n","Topic 22: 0.099*\"exist\" + 0.050*\"claim\" + 0.049*\"evid\" + 0.026*\"scienc\" + 0.022*\"prove\" + 0.021*\"scientif\" + 0.018*\"object\" + 0.017*\"univers\" + 0.016*\"theori\" + 0.014*\"truth\" + 0.014*\"question\" + 0.012*\"fact\" + 0.012*\"observ\" + 0.012*\"explain\" + 0.011*\"proof\" + 0.010*\"support\" + 0.009*\"natur\" + 0.008*\"physic\" + 0.008*\"answer\" + 0.008*\"begin\" + 0.007*\"reason\" + 0.007*\"caus\" + 0.007*\"time\" + 0.006*\"empir\" + 0.006*\"scientist\" + 0.006*\"experi\" + 0.006*\"provid\" + 0.006*\"base\" + 0.006*\"suggest\" + 0.006*\"assert\" + 0.006*\"explan\" + 0.006*\"mean\" + 0.006*\"subject\" + 0.006*\"model\" + 0.005*\"point\" + 0.005*\"philosoph\" + 0.005*\"method\" + 0.005*\"accept\" + 0.005*\"understand\" + 0.005*\"possibl\"\n","Topic 23: 0.065*\"languag\" + 0.064*\"area\" + 0.040*\"realiti\" + 0.036*\"line\" + 0.035*\"speak\" + 0.031*\"rise\" + 0.018*\"draw\" + 0.017*\"virtual\" + 0.015*\"middl\" + 0.015*\"locat\" + 0.014*\"root\" + 0.014*\"classic\" + 0.012*\"contact\" + 0.011*\"chines\" + 0.011*\"shooter\" + 0.011*\"speaker\" + 0.011*\"english\" + 0.010*\"age\" + 0.010*\"empathi\" + 0.010*\"censor\" + 0.009*\"world\" + 0.008*\"rural\" + 0.008*\"drastic\" + 0.008*\"east\" + 0.008*\"korean\" + 0.007*\"nativ\" + 0.007*\"split\" + 0.007*\"back\" + 0.007*\"serious\" + 0.006*\"geograph\" + 0.006*\"esperanto\" + 0.006*\"unreason\" + 0.006*\"inappropri\" + 0.006*\"retir\" + 0.006*\"inconsist\" + 0.005*\"furthermor\" + 0.005*\"word\" + 0.005*\"french\" + 0.005*\"predomin\" + 0.005*\"ahead\"\n","Topic 24: 0.070*\"church\" + 0.033*\"stand\" + 0.029*\"cathol\" + 0.028*\"protest\" + 0.025*\"player\" + 0.024*\"period\" + 0.021*\"time\" + 0.021*\"friend\" + 0.020*\"charact\" + 0.018*\"amount\" + 0.013*\"search\" + 0.013*\"room\" + 0.012*\"wait\" + 0.011*\"narrat\" + 0.011*\"buy\" + 0.011*\"mobil\" + 0.011*\"devic\" + 0.010*\"phone\" + 0.009*\"audienc\" + 0.009*\"googl\" + 0.009*\"felt\" + 0.008*\"stick\" + 0.008*\"pope\" + 0.008*\"inspir\" + 0.007*\"lock\" + 0.007*\"evangel\" + 0.007*\"fraud\" + 0.007*\"night\" + 0.007*\"warrant\" + 0.007*\"footbal\" + 0.006*\"screen\" + 0.006*\"endur\" + 0.006*\"allow\" + 0.006*\"appl\" + 0.006*\"prayer\" + 0.006*\"help\" + 0.006*\"pray\" + 0.005*\"irrespons\" + 0.005*\"season\" + 0.005*\"earlier\"\n","Topic 25: 0.043*\"govern\" + 0.041*\"money\" + 0.027*\"servic\" + 0.024*\"provid\" + 0.022*\"regul\" + 0.021*\"spend\" + 0.021*\"compani\" + 0.021*\"market\" + 0.020*\"busi\" + 0.020*\"fund\" + 0.020*\"access\" + 0.017*\"welfar\" + 0.016*\"privat\" + 0.016*\"tax\" + 0.014*\"pay\" + 0.014*\"public\" + 0.014*\"need\" + 0.013*\"incent\" + 0.013*\"profit\" + 0.012*\"financi\" + 0.010*\"cost\" + 0.009*\"price\" + 0.009*\"afford\" + 0.008*\"program\" + 0.008*\"sell\" + 0.008*\"incom\" + 0.007*\"like\" + 0.007*\"corpor\" + 0.007*\"expens\" + 0.007*\"benefit\" + 0.007*\"healthcar\" + 0.007*\"allow\" + 0.007*\"offer\" + 0.006*\"mean\" + 0.006*\"taxat\" + 0.006*\"creat\" + 0.006*\"custom\" + 0.006*\"revenu\" + 0.006*\"free\" + 0.006*\"internet\"\n","Topic 26: 0.036*\"industri\" + 0.024*\"robot\" + 0.023*\"onlin\" + 0.023*\"content\" + 0.021*\"articl\" + 0.020*\"creation\" + 0.017*\"agreement\" + 0.015*\"british\" + 0.015*\"publish\" + 0.015*\"respond\" + 0.015*\"review\" + 0.014*\"transact\" + 0.013*\"artist\" + 0.013*\"cite\" + 0.013*\"like\" + 0.012*\"display\" + 0.012*\"peer\" + 0.012*\"digit\" + 0.011*\"plenti\" + 0.011*\"note\" + 0.011*\"read\" + 0.011*\"anonym\" + 0.011*\"bigger\" + 0.009*\"comput\" + 0.009*\"websit\" + 0.008*\"revolut\" + 0.008*\"kingdom\" + 0.008*\"monitor\" + 0.008*\"messag\" + 0.007*\"music\" + 0.007*\"ireland\" + 0.006*\"stock\" + 0.006*\"replic\" + 0.006*\"england\" + 0.006*\"uncomfort\" + 0.006*\"format\" + 0.006*\"internet\" + 0.006*\"northern\" + 0.006*\"master\" + 0.005*\"list\"\n","Topic 27: 0.028*\"christian\" + 0.025*\"histor\" + 0.025*\"book\" + 0.024*\"jesus\" + 0.024*\"word\" + 0.022*\"write\" + 0.021*\"bibl\" + 0.019*\"author\" + 0.014*\"stori\" + 0.014*\"refer\" + 0.012*\"interpret\" + 0.012*\"monument\" + 0.011*\"say\" + 0.010*\"text\" + 0.010*\"document\" + 0.010*\"origin\" + 0.009*\"mention\" + 0.008*\"statu\" + 0.008*\"command\" + 0.008*\"time\" + 0.008*\"ancient\" + 0.008*\"dead\" + 0.007*\"liter\" + 0.007*\"record\" + 0.007*\"translat\" + 0.007*\"roman\" + 0.007*\"divin\" + 0.007*\"tell\" + 0.007*\"john\" + 0.006*\"king\" + 0.006*\"context\" + 0.006*\"contain\" + 0.006*\"describ\" + 0.006*\"mark\" + 0.006*\"testament\" + 0.006*\"claim\" + 0.005*\"christ\" + 0.005*\"version\" + 0.005*\"greek\" + 0.005*\"teach\"\n","Topic 28: 0.197*\"human\" + 0.094*\"life\" + 0.028*\"univers\" + 0.019*\"anim\" + 0.018*\"intellig\" + 0.016*\"natur\" + 0.016*\"capabl\" + 0.014*\"be\" + 0.014*\"live\" + 0.014*\"conscious\" + 0.012*\"creat\" + 0.012*\"qualiti\" + 0.011*\"capac\" + 0.008*\"design\" + 0.008*\"understand\" + 0.008*\"develop\" + 0.008*\"like\" + 0.007*\"evolv\" + 0.007*\"speci\" + 0.007*\"mean\" + 0.007*\"evolut\" + 0.007*\"experi\" + 0.007*\"abil\" + 0.007*\"valu\" + 0.006*\"simul\" + 0.006*\"artifici\" + 0.006*\"consid\" + 0.005*\"machin\" + 0.005*\"self\" + 0.005*\"possibl\" + 0.005*\"think\" + 0.005*\"mind\" + 0.005*\"cognit\" + 0.005*\"communic\" + 0.005*\"abl\" + 0.005*\"brain\" + 0.005*\"need\" + 0.004*\"potenti\" + 0.004*\"worth\" + 0.004*\"similar\"\n","Topic 29: 0.082*\"idea\" + 0.055*\"violenc\" + 0.049*\"speech\" + 0.030*\"express\" + 0.028*\"class\" + 0.025*\"concept\" + 0.022*\"hate\" + 0.019*\"perspect\" + 0.019*\"free\" + 0.018*\"meaning\" + 0.017*\"opinion\" + 0.016*\"infinit\" + 0.011*\"nazi\" + 0.010*\"god\" + 0.010*\"ground\" + 0.010*\"hear\" + 0.010*\"etern\" + 0.009*\"appeal\" + 0.009*\"limit\" + 0.009*\"offens\" + 0.009*\"voic\" + 0.008*\"date\" + 0.008*\"forbid\" + 0.008*\"time\" + 0.008*\"think\" + 0.007*\"side\" + 0.007*\"allow\" + 0.007*\"view\" + 0.007*\"censorship\" + 0.007*\"irrat\" + 0.006*\"univers\" + 0.006*\"whilst\" + 0.006*\"finit\" + 0.006*\"discours\" + 0.006*\"word\" + 0.006*\"consid\" + 0.005*\"middl\" + 0.005*\"number\" + 0.005*\"spread\" + 0.005*\"discuss\"\n","Topic 30: 0.115*\"moral\" + 0.072*\"polit\" + 0.057*\"law\" + 0.028*\"standard\" + 0.026*\"wrong\" + 0.024*\"correct\" + 0.024*\"republican\" + 0.023*\"motiv\" + 0.016*\"conserv\" + 0.014*\"view\" + 0.014*\"ethic\" + 0.013*\"ideolog\" + 0.013*\"oblig\" + 0.011*\"parti\" + 0.011*\"subject\" + 0.010*\"pizza\" + 0.008*\"pineappl\" + 0.008*\"agre\" + 0.008*\"appli\" + 0.008*\"consid\" + 0.007*\"base\" + 0.007*\"fetus\" + 0.007*\"person\" + 0.007*\"object\" + 0.006*\"agenda\" + 0.006*\"framework\" + 0.006*\"hold\" + 0.006*\"everybodi\" + 0.006*\"fine\" + 0.006*\"issu\" + 0.005*\"disagre\" + 0.005*\"support\" + 0.005*\"wing\" + 0.005*\"creat\" + 0.005*\"mean\" + 0.005*\"comment\" + 0.005*\"stanc\" + 0.004*\"represent\" + 0.004*\"surrogaci\" + 0.004*\"hotel\"\n","Topic 31: 0.093*\"forc\" + 0.044*\"militari\" + 0.035*\"secur\" + 0.035*\"fight\" + 0.023*\"arm\" + 0.023*\"target\" + 0.022*\"defens\" + 0.016*\"akm\" + 0.016*\"bank\" + 0.016*\"oper\" + 0.015*\"war\" + 0.014*\"soldier\" + 0.013*\"civilian\" + 0.013*\"armi\" + 0.012*\"weapon\" + 0.011*\"extra\" + 0.010*\"join\" + 0.009*\"union\" + 0.009*\"enemi\" + 0.009*\"attack\" + 0.008*\"nuditi\" + 0.008*\"order\" + 0.007*\"withdraw\" + 0.007*\"ship\" + 0.006*\"grind\" + 0.006*\"team\" + 0.006*\"surrend\" + 0.006*\"unpaid\" + 0.006*\"militia\" + 0.006*\"soviet\" + 0.006*\"britain\" + 0.006*\"strike\" + 0.005*\"graduat\" + 0.005*\"deploy\" + 0.005*\"kill\" + 0.005*\"nato\" + 0.005*\"rebel\" + 0.005*\"self\" + 0.004*\"end\" + 0.004*\"warfar\"\n","Topic 32: 0.334*\"peopl\" + 0.060*\"live\" + 0.039*\"want\" + 0.034*\"feel\" + 0.025*\"like\" + 0.018*\"away\" + 0.015*\"person\" + 0.014*\"need\" + 0.013*\"poor\" + 0.010*\"help\" + 0.010*\"abl\" + 0.010*\"enjoy\" + 0.009*\"wealthi\" + 0.009*\"mean\" + 0.009*\"will\" + 0.008*\"better\" + 0.008*\"thing\" + 0.008*\"know\" + 0.007*\"allow\" + 0.007*\"choos\" + 0.007*\"hurt\" + 0.006*\"take\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"fewer\" + 0.006*\"give\" + 0.005*\"have\" + 0.005*\"place\" + 0.005*\"experi\" + 0.004*\"go\" + 0.004*\"instead\" + 0.004*\"stop\" + 0.004*\"sacrific\" + 0.004*\"leav\" + 0.004*\"lead\" + 0.004*\"revers\" + 0.004*\"reason\" + 0.004*\"get\" + 0.004*\"longer\" + 0.003*\"prevent\"\n","Topic 33: 0.094*\"group\" + 0.081*\"polici\" + 0.045*\"minor\" + 0.041*\"polic\" + 0.022*\"implement\" + 0.020*\"offic\" + 0.018*\"address\" + 0.017*\"action\" + 0.015*\"liber\" + 0.014*\"legitim\" + 0.014*\"racial\" + 0.014*\"affirm\" + 0.013*\"prohibit\" + 0.012*\"govern\" + 0.011*\"oppress\" + 0.010*\"attent\" + 0.010*\"issu\" + 0.010*\"ethnic\" + 0.010*\"racism\" + 0.009*\"support\" + 0.009*\"sanction\" + 0.009*\"public\" + 0.008*\"toler\" + 0.008*\"enforc\" + 0.008*\"like\" + 0.007*\"effect\" + 0.007*\"donor\" + 0.007*\"zero\" + 0.007*\"margin\" + 0.007*\"direct\" + 0.006*\"elit\" + 0.006*\"specif\" + 0.006*\"send\" + 0.006*\"enact\" + 0.006*\"discrimin\" + 0.005*\"member\" + 0.005*\"indirect\" + 0.005*\"disappear\" + 0.005*\"tension\" + 0.005*\"anti\"\n","Topic 34: 0.114*\"year\" + 0.050*\"million\" + 0.035*\"averag\" + 0.022*\"past\" + 0.022*\"time\" + 0.021*\"total\" + 0.020*\"half\" + 0.015*\"month\" + 0.015*\"decad\" + 0.014*\"highest\" + 0.014*\"drop\" + 0.013*\"salari\" + 0.013*\"introduc\" + 0.013*\"obama\" + 0.012*\"track\" + 0.011*\"failur\" + 0.011*\"number\" + 0.011*\"trend\" + 0.010*\"commerci\" + 0.010*\"releas\" + 0.009*\"dollar\" + 0.008*\"pick\" + 0.007*\"recent\" + 0.007*\"pull\" + 0.007*\"near\" + 0.007*\"africa\" + 0.007*\"techniqu\" + 0.007*\"lowest\" + 0.007*\"estim\" + 0.007*\"thousand\" + 0.006*\"period\" + 0.006*\"dedic\" + 0.006*\"show\" + 0.006*\"journal\" + 0.006*\"spectrum\" + 0.005*\"record\" + 0.005*\"take\" + 0.005*\"underground\" + 0.004*\"ongo\" + 0.004*\"start\"\n","Topic 35: 0.063*\"definit\" + 0.046*\"consent\" + 0.045*\"knowledg\" + 0.044*\"defin\" + 0.041*\"properti\" + 0.034*\"debat\" + 0.032*\"immor\" + 0.022*\"talk\" + 0.022*\"omnipot\" + 0.021*\"adult\" + 0.017*\"theft\" + 0.016*\"contract\" + 0.014*\"comfort\" + 0.013*\"monarchi\" + 0.012*\"mean\" + 0.012*\"pure\" + 0.011*\"person\" + 0.011*\"fulli\" + 0.010*\"omnisci\" + 0.010*\"thing\" + 0.009*\"mayb\" + 0.008*\"consid\" + 0.008*\"clear\" + 0.007*\"condemn\" + 0.007*\"intellectu\" + 0.007*\"consensu\" + 0.007*\"somebodi\" + 0.007*\"boundari\" + 0.007*\"lot\" + 0.006*\"concept\" + 0.006*\"possess\" + 0.006*\"ordinari\" + 0.006*\"meaningless\" + 0.006*\"lack\" + 0.006*\"understand\" + 0.005*\"thesi\" + 0.005*\"includ\" + 0.005*\"classic\" + 0.005*\"have\" + 0.005*\"couldn\"\n","Topic 36: 0.122*\"make\" + 0.072*\"decis\" + 0.045*\"process\" + 0.022*\"emot\" + 0.020*\"hous\" + 0.020*\"intent\" + 0.020*\"tortur\" + 0.018*\"behaviour\" + 0.018*\"kind\" + 0.017*\"easier\" + 0.017*\"complex\" + 0.014*\"applic\" + 0.013*\"reward\" + 0.012*\"difficult\" + 0.011*\"simpl\" + 0.011*\"like\" + 0.009*\"automat\" + 0.009*\"creator\" + 0.009*\"behavior\" + 0.009*\"person\" + 0.008*\"harder\" + 0.008*\"negoti\" + 0.007*\"precis\" + 0.006*\"control\" + 0.006*\"understand\" + 0.005*\"evalu\" + 0.005*\"sens\" + 0.005*\"signal\" + 0.005*\"compon\" + 0.005*\"manner\" + 0.005*\"influenc\" + 0.005*\"complic\" + 0.005*\"action\" + 0.005*\"ration\" + 0.005*\"involv\" + 0.005*\"base\" + 0.005*\"mathemat\" + 0.005*\"gene\" + 0.004*\"intrins\" + 0.004*\"decid\"\n","Topic 37: 0.157*\"religion\" + 0.095*\"religi\" + 0.065*\"believ\" + 0.057*\"belief\" + 0.029*\"faith\" + 0.024*\"think\" + 0.017*\"christian\" + 0.016*\"secular\" + 0.012*\"follow\" + 0.012*\"practic\" + 0.011*\"teach\" + 0.010*\"base\" + 0.010*\"exempt\" + 0.008*\"convers\" + 0.008*\"reason\" + 0.007*\"islam\" + 0.007*\"promot\" + 0.007*\"adher\" + 0.007*\"ethic\" + 0.007*\"atheist\" + 0.007*\"exampl\" + 0.006*\"spiritu\" + 0.006*\"contracept\" + 0.006*\"heaven\" + 0.006*\"view\" + 0.006*\"myth\" + 0.006*\"world\" + 0.006*\"thing\" + 0.005*\"conflict\" + 0.005*\"mainstream\" + 0.005*\"caus\" + 0.005*\"provid\" + 0.005*\"philosophi\" + 0.005*\"atroc\" + 0.005*\"person\" + 0.004*\"lead\" + 0.004*\"excus\" + 0.004*\"control\" + 0.004*\"hold\" + 0.004*\"doctrin\"\n","Topic 38: 0.109*\"american\" + 0.071*\"white\" + 0.067*\"natur\" + 0.060*\"black\" + 0.049*\"slaveri\" + 0.033*\"race\" + 0.029*\"slave\" + 0.027*\"select\" + 0.016*\"uniqu\" + 0.016*\"african\" + 0.010*\"inherit\" + 0.010*\"autonomi\" + 0.009*\"racist\" + 0.009*\"today\" + 0.008*\"bodili\" + 0.007*\"colour\" + 0.007*\"disast\" + 0.007*\"descend\" + 0.007*\"conscript\" + 0.007*\"supremacist\" + 0.007*\"let\" + 0.007*\"chain\" + 0.007*\"skin\" + 0.007*\"exampl\" + 0.007*\"occup\" + 0.007*\"america\" + 0.006*\"ethic\" + 0.006*\"ancestor\" + 0.006*\"twice\" + 0.005*\"racism\" + 0.005*\"accumul\" + 0.005*\"feasibl\" + 0.005*\"asian\" + 0.005*\"shape\" + 0.005*\"urban\" + 0.004*\"enslav\" + 0.004*\"privileg\" + 0.004*\"proceed\" + 0.004*\"cosmet\" + 0.004*\"supremaci\"\n","Topic 39: 0.061*\"health\" + 0.035*\"care\" + 0.034*\"mental\" + 0.031*\"condit\" + 0.031*\"negat\" + 0.030*\"medic\" + 0.022*\"effect\" + 0.021*\"treatment\" + 0.017*\"psycholog\" + 0.015*\"patient\" + 0.014*\"improv\" + 0.013*\"impact\" + 0.013*\"test\" + 0.013*\"research\" + 0.012*\"payment\" + 0.012*\"ill\" + 0.012*\"alcohol\" + 0.011*\"diseas\" + 0.010*\"treat\" + 0.010*\"profession\" + 0.009*\"physic\" + 0.009*\"result\" + 0.009*\"procedur\" + 0.009*\"trigger\" + 0.008*\"warn\" + 0.008*\"medicin\" + 0.008*\"studi\" + 0.008*\"disord\" + 0.007*\"caus\" + 0.007*\"like\" + 0.007*\"surgeri\" + 0.007*\"posit\" + 0.007*\"risk\" + 0.006*\"issu\" + 0.006*\"associ\" + 0.006*\"benefit\" + 0.006*\"pleasur\" + 0.006*\"suffer\" + 0.006*\"hospit\" + 0.006*\"experi\"\n","Topic 40: 0.116*\"children\" + 0.080*\"parent\" + 0.064*\"child\" + 0.055*\"famili\" + 0.044*\"abort\" + 0.037*\"abus\" + 0.023*\"bear\" + 0.018*\"mother\" + 0.014*\"disabl\" + 0.013*\"rais\" + 0.012*\"pregnanc\" + 0.011*\"babi\" + 0.011*\"marri\" + 0.010*\"father\" + 0.009*\"have\" + 0.009*\"care\" + 0.008*\"stigma\" + 0.008*\"life\" + 0.008*\"woman\" + 0.008*\"respons\" + 0.007*\"allow\" + 0.007*\"leav\" + 0.007*\"case\" + 0.007*\"like\" + 0.006*\"attach\" + 0.006*\"support\" + 0.006*\"infant\" + 0.006*\"want\" + 0.005*\"choos\" + 0.005*\"permiss\" + 0.005*\"young\" + 0.005*\"abl\" + 0.005*\"pregnant\" + 0.005*\"home\" + 0.005*\"reason\" + 0.005*\"determinist\" + 0.004*\"unwant\" + 0.004*\"gang\" + 0.004*\"time\" + 0.004*\"need\"\n","Topic 41: 0.060*\"argument\" + 0.056*\"true\" + 0.046*\"know\" + 0.041*\"logic\" + 0.036*\"suffer\" + 0.035*\"assum\" + 0.029*\"imposs\" + 0.021*\"fals\" + 0.018*\"claim\" + 0.018*\"pain\" + 0.017*\"reason\" + 0.015*\"statement\" + 0.013*\"ignor\" + 0.013*\"valid\" + 0.013*\"possibl\" + 0.012*\"assumpt\" + 0.010*\"case\" + 0.009*\"fallaci\" + 0.009*\"mean\" + 0.008*\"fact\" + 0.008*\"necess\" + 0.008*\"actual\" + 0.008*\"imagin\" + 0.008*\"thing\" + 0.008*\"absenc\" + 0.007*\"caus\" + 0.007*\"premis\" + 0.006*\"ration\" + 0.006*\"point\" + 0.006*\"interact\" + 0.005*\"say\" + 0.005*\"analog\" + 0.005*\"sound\" + 0.005*\"prove\" + 0.005*\"base\" + 0.005*\"complet\" + 0.005*\"contradictori\" + 0.004*\"conclus\" + 0.004*\"simpli\" + 0.004*\"experi\"\n","Topic 42: 0.146*\"work\" + 0.054*\"worker\" + 0.046*\"long\" + 0.045*\"term\" + 0.044*\"cost\" + 0.037*\"reduc\" + 0.029*\"job\" + 0.025*\"employ\" + 0.023*\"product\" + 0.014*\"time\" + 0.013*\"elimin\" + 0.012*\"short\" + 0.012*\"labor\" + 0.011*\"employe\" + 0.011*\"traffic\" + 0.011*\"stress\" + 0.010*\"wage\" + 0.010*\"autom\" + 0.009*\"minimum\" + 0.007*\"licens\" + 0.007*\"effect\" + 0.007*\"creat\" + 0.007*\"labour\" + 0.007*\"combat\" + 0.007*\"high\" + 0.007*\"pay\" + 0.006*\"requir\" + 0.006*\"industri\" + 0.006*\"compani\" + 0.006*\"hour\" + 0.006*\"benefit\" + 0.006*\"need\" + 0.005*\"hard\" + 0.005*\"obes\" + 0.005*\"like\" + 0.005*\"mean\" + 0.005*\"hire\" + 0.004*\"provid\" + 0.004*\"develop\" + 0.004*\"skill\"\n","Topic 43: 0.105*\"trump\" + 0.036*\"weapon\" + 0.036*\"presid\" + 0.027*\"nuclear\" + 0.023*\"firearm\" + 0.022*\"north\" + 0.021*\"administr\" + 0.019*\"america\" + 0.019*\"bomb\" + 0.017*\"korea\" + 0.017*\"repar\" + 0.014*\"japan\" + 0.014*\"owner\" + 0.014*\"deal\" + 0.013*\"australia\" + 0.012*\"south\" + 0.012*\"japanes\" + 0.011*\"equival\" + 0.010*\"impeach\" + 0.010*\"minist\" + 0.008*\"smoke\" + 0.008*\"prime\" + 0.008*\"alli\" + 0.007*\"iran\" + 0.007*\"support\" + 0.007*\"isi\" + 0.006*\"campaign\" + 0.006*\"pornographi\" + 0.006*\"russia\" + 0.006*\"porn\" + 0.006*\"island\" + 0.006*\"accident\" + 0.006*\"cannabi\" + 0.006*\"compens\" + 0.005*\"door\" + 0.005*\"like\" + 0.004*\"obstruct\" + 0.004*\"sale\" + 0.004*\"promis\" + 0.004*\"open\"\n","Topic 44: 0.066*\"great\" + 0.048*\"altern\" + 0.039*\"refuge\" + 0.035*\"corrupt\" + 0.033*\"propos\" + 0.029*\"mechan\" + 0.019*\"host\" + 0.015*\"ought\" + 0.015*\"viabl\" + 0.014*\"greatest\" + 0.014*\"thousand\" + 0.013*\"countri\" + 0.013*\"overwhelm\" + 0.013*\"cod\" + 0.012*\"terribl\" + 0.011*\"depart\" + 0.011*\"hundr\" + 0.010*\"segreg\" + 0.010*\"home\" + 0.009*\"exampl\" + 0.009*\"arriv\" + 0.009*\"deepli\" + 0.009*\"distress\" + 0.008*\"fit\" + 0.008*\"suspect\" + 0.008*\"communism\" + 0.007*\"termin\" + 0.007*\"incoher\" + 0.007*\"green\" + 0.007*\"take\" + 0.007*\"tackl\" + 0.007*\"nutrit\" + 0.007*\"wall\" + 0.006*\"librari\" + 0.006*\"coal\" + 0.006*\"nafta\" + 0.006*\"accept\" + 0.006*\"unrel\" + 0.005*\"camp\" + 0.005*\"cope\"\n","Topic 45: 0.066*\"problem\" + 0.060*\"popul\" + 0.036*\"technolog\" + 0.033*\"futur\" + 0.029*\"resourc\" + 0.023*\"global\" + 0.018*\"progress\" + 0.017*\"world\" + 0.017*\"advanc\" + 0.015*\"replac\" + 0.015*\"develop\" + 0.014*\"solut\" + 0.013*\"solv\" + 0.013*\"need\" + 0.013*\"scale\" + 0.011*\"sustain\" + 0.011*\"issu\" + 0.009*\"current\" + 0.009*\"larg\" + 0.009*\"size\" + 0.009*\"like\" + 0.008*\"larger\" + 0.008*\"massiv\" + 0.008*\"innov\" + 0.008*\"overpopul\" + 0.008*\"caus\" + 0.007*\"creat\" + 0.007*\"time\" + 0.007*\"priest\" + 0.007*\"better\" + 0.006*\"adapt\" + 0.006*\"help\" + 0.006*\"smaller\" + 0.006*\"distribut\" + 0.005*\"lead\" + 0.005*\"slow\" + 0.005*\"continu\" + 0.005*\"system\" + 0.005*\"effect\" + 0.005*\"requir\"\n","Topic 46: 0.058*\"studi\" + 0.050*\"report\" + 0.032*\"love\" + 0.031*\"billion\" + 0.025*\"advantag\" + 0.024*\"young\" + 0.020*\"accord\" + 0.019*\"percent\" + 0.017*\"disadvantag\" + 0.016*\"survey\" + 0.015*\"tran\" + 0.014*\"compet\" + 0.014*\"hell\" + 0.013*\"compar\" + 0.013*\"girl\" + 0.013*\"estim\" + 0.013*\"research\" + 0.013*\"transgend\" + 0.012*\"day\" + 0.011*\"show\" + 0.011*\"conduct\" + 0.011*\"center\" + 0.010*\"categori\" + 0.009*\"athlet\" + 0.008*\"coloni\" + 0.008*\"approxim\" + 0.008*\"confid\" + 0.008*\"suggest\" + 0.007*\"transit\" + 0.007*\"closer\" + 0.007*\"mar\" + 0.006*\"annual\" + 0.006*\"self\" + 0.006*\"scope\" + 0.006*\"quot\" + 0.006*\"match\" + 0.006*\"boy\" + 0.006*\"legaci\" + 0.006*\"lifetim\" + 0.006*\"adult\"\n","Topic 47: 0.117*\"major\" + 0.034*\"germani\" + 0.027*\"vast\" + 0.021*\"india\" + 0.020*\"muslim\" + 0.019*\"europ\" + 0.016*\"countri\" + 0.016*\"german\" + 0.015*\"jew\" + 0.014*\"tie\" + 0.013*\"financ\" + 0.012*\"spain\" + 0.012*\"regist\" + 0.012*\"franc\" + 0.009*\"support\" + 0.009*\"mexico\" + 0.009*\"recognit\" + 0.008*\"canada\" + 0.008*\"jewish\" + 0.008*\"subsequ\" + 0.008*\"delay\" + 0.007*\"sleep\" + 0.007*\"feminin\" + 0.006*\"strateg\" + 0.006*\"expans\" + 0.006*\"exampl\" + 0.006*\"anti\" + 0.006*\"itali\" + 0.006*\"indian\" + 0.006*\"world\" + 0.006*\"riski\" + 0.006*\"mexican\" + 0.006*\"spous\" + 0.006*\"modi\" + 0.005*\"hungari\" + 0.005*\"pilot\" + 0.005*\"greec\" + 0.005*\"super\" + 0.005*\"vatican\" + 0.005*\"quarter\"\n","Topic 48: 0.134*\"differ\" + 0.093*\"cultur\" + 0.082*\"valu\" + 0.046*\"common\" + 0.034*\"share\" + 0.034*\"tradit\" + 0.020*\"western\" + 0.017*\"divers\" + 0.014*\"norm\" + 0.014*\"centuri\" + 0.011*\"appropri\" + 0.010*\"core\" + 0.009*\"integr\" + 0.008*\"foster\" + 0.008*\"practic\" + 0.007*\"world\" + 0.007*\"deserv\" + 0.006*\"countri\" + 0.006*\"fact\" + 0.006*\"preval\" + 0.006*\"accept\" + 0.005*\"tenet\" + 0.005*\"form\" + 0.005*\"modern\" + 0.005*\"consid\" + 0.005*\"conform\" + 0.005*\"mean\" + 0.005*\"respect\" + 0.005*\"evolutionari\" + 0.005*\"unjust\" + 0.004*\"heritag\" + 0.004*\"creat\" + 0.004*\"featur\" + 0.004*\"embrac\" + 0.004*\"gambl\" + 0.004*\"way\" + 0.004*\"exampl\" + 0.004*\"today\" + 0.004*\"view\" + 0.004*\"similar\"\n","Topic 49: 0.084*\"histori\" + 0.055*\"surviv\" + 0.026*\"enabl\" + 0.024*\"preserv\" + 0.021*\"trait\" + 0.020*\"breed\" + 0.018*\"hide\" + 0.017*\"dog\" + 0.017*\"superior\" + 0.015*\"memori\" + 0.014*\"wild\" + 0.013*\"weight\" + 0.013*\"facilit\" + 0.012*\"celebr\" + 0.012*\"convinc\" + 0.012*\"bond\" + 0.011*\"hope\" + 0.009*\"depriv\" + 0.009*\"magic\" + 0.009*\"heart\" + 0.008*\"world\" + 0.008*\"art\" + 0.008*\"touch\" + 0.008*\"rememb\" + 0.008*\"pride\" + 0.008*\"farmer\" + 0.008*\"habitat\" + 0.008*\"museum\" + 0.008*\"realiz\" + 0.007*\"wizard\" + 0.007*\"classroom\" + 0.007*\"tree\" + 0.007*\"classifi\" + 0.007*\"like\" + 0.007*\"immort\" + 0.007*\"unrealist\" + 0.006*\"cat\" + 0.006*\"fli\" + 0.006*\"evolutionari\" + 0.006*\"remov\"\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# Get entities for each sentence and add for the topics\\ndictionary = gensim.corpora.Dictionary(all_docs)\\nfor idx in tqdm(range(len(all_docs)), ascii=True):\\n  doc = all_docs[idx]\\n  sent = all_sents[idx]\\n  corpus = [dictionary.doc2bow(doc)]\\n  top_topics = (\\n      lda_model.get_document_topics(corpus, minimum_probability=0.0)\\n  )\\n  # Pick top topic for adding entities\\n  top_topic = sorted(top_topics[0], key=lambda x: x[1], reverse=True)[0]\\n  top_topic_id = top_topic[0]\\n\\n  entities = []\\n  tk_sents = sent_tokenize(sent)\\n  for ss in tk_sents:\\n    e1, e2 = extract_entities(ss)\\n    entities.append((e1, e2))\\n  \\n  # Add to the current topic as a single entry\\n  topic_wise_entities[top_topic_id].append(entities)\\n\\ntopic_entities_json = os.path.join(drive_path, 'kialo_topics', 'topic_entities.json')\\nwith open(topic_entities_json, 'w') as f:\\n  json.dump(topic_wise_entities, f)\\n\""]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["drive_path = '/content/drive/MyDrive'\n","topics_path = os.path.join(drive_path, 'topics_overall')\n","\n","import spacy\n","#nlp = spacy.load('en_core_web_sm')\n","\n","from spacy.tokens import Span\n","from spacy.matcher import Matcher\n","\n","nltk.download('punkt')\n","\n","topic_model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","lda_model = gensim.models.LdaMulticore.load(topic_model_path)\n","\n","def extract_entities(sents):\n","   global nlp\n","   # chunk one\n","   enti_one = \"\"\n","   enti_two = \"\"\n","  \n","   dep_prev_token = \"\" # dependency tag of previous token in sentence\n","  \n","   txt_prev_token = \"\" # previous token in sentence\n","  \n","   prefix = \"\"\n","   modifier = \"\"\n","  \n","   for tokn in nlp(sents):\n","       # chunk two\n","       ## move to next token if token is punctuation\n","      \n","       if tokn.dep_ != \"punct\":\n","           #  check if token is compound word or not\n","           if tokn.dep_ == \"compound\":\n","               prefix = tokn.text\n","               # add the current word to it if the previous word is 'compound’\n","               if dep_prev_token == \"compound\":\n","                   prefix = txt_prev_token + \" \"+ tokn.text\n","                  \n","           # verify if token is modifier or not\n","           if tokn.dep_.endswith(\"mod\") == True:\n","               modifier = tokn.text\n","               # add it to the current word if the previous word is 'compound'\n","               if dep_prev_token == \"compound\":\n","                   modifier = txt_prev_token + \" \"+ tokn.text\n","                  \n","           # chunk3\n","           if tokn.dep_.find(\"subj\") == True:\n","               enti_one = modifier +\" \"+ prefix + \" \"+ tokn.text\n","               prefix = \"\"\n","               modifier = \"\"\n","               dep_prev_token = \"\"\n","               txt_prev_token = \"\"\n","              \n","           # chunk4\n","           if tokn.dep_.find(\"obj\") == True:\n","               enti_two = modifier +\" \"+ prefix +\" \"+ tokn.text\n","              \n","           # chunk 5\n","           # update variable\n","           dep_prev_token = tokn.dep_\n","           txt_prev_token = tokn.text\n","          \n","   return [enti_one.strip(), enti_two.strip()]\n","\n","\n","os.makedirs(topics_path, exist_ok=True)\n","\n","\"\"\"\n","for _, (k, v) in enumerate(tqdm(models.items(), ascii=True)):\n","  os.makedirs(os.path.join(topics_path, k), exist_ok=True)\n","  fname_pro = models[k]['pro_save_name']\n","  if fname_pro:\n","    fname_pro = fname_pro.replace('models', os.path.join(drive_path, 'models'))\n","    lda_model = gensim.models.LdaMulticore.load(fname_pro)\n","    for idx, topic in lda_model.print_topics(num_words=20):\n","      topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","      topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","      topic_words = [\n","          re.sub(r'http\\S+', '', min(x, key=len)).strip()\n","          if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)\n","      ]\n","      with open(os.path.join(topics_path, k, f'pro_{topic_words[0]}_{idx}.txt'), 'w') as f:\n","        for w in topic_words[:-1]:\n","          f.write(f'{w}\\n')\n","        f.write(f'{topic_words[-1]}')\n","  fname_con = models[k]['con_save_name']\n","  if fname_con:\n","    fname_con = fname_con.replace('models', os.path.join(drive_path, 'models'))\n","    lda_model = gensim.models.LdaMulticore.load(fname_con)\n","    for idx, topic in lda_model.print_topics(num_words=20):\n","      topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","      topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","      topic_words = [\n","          re.sub(r'http\\S+', '', min(x, key=len)).strip().replace('/', '_')\n","          if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)\n","      ]\n","      with open(os.path.join(topics_path, k, f'con_{topic_words[0]}_{idx}.txt'), 'w') as f:\n","        for w in topic_words[:-1]:\n","          f.write(f'{w}\\n')\n","        f.write(f'{topic_words[-1]}')\n","\"\"\"\n","\n","topic_wise_entities = {\n","    i: [] for i in range(num_topics)\n","}\n","topic_wise_words = {\n","    i: [] for i in range(num_topics)\n","}\n","\n","from nltk.tokenize import sent_tokenize\n","\n","# Get top-40 words for each topic\n","for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=40):\n","  topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","  topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","  topic_words = [min(x, key=len) if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)]\n","  print(f\"Topic {idx}: {topic}\")\n","  topic_wise_words[idx] = topic_words\n","\n","# print(topic_wise_words)\n","topic_words_json = os.path.join(drive_path, 'kialo_topics', 'topic_words.json')\n","with open(topic_words_json, 'w') as f:\n","  json.dump(topic_wise_words, f)\n","\n","\"\"\"\n","# Get entities for each sentence and add for the topics\n","dictionary = gensim.corpora.Dictionary(all_docs)\n","for idx in tqdm(range(len(all_docs)), ascii=True):\n","  doc = all_docs[idx]\n","  sent = all_sents[idx]\n","  corpus = [dictionary.doc2bow(doc)]\n","  top_topics = (\n","      lda_model.get_document_topics(corpus, minimum_probability=0.0)\n","  )\n","  # Pick top topic for adding entities\n","  top_topic = sorted(top_topics[0], key=lambda x: x[1], reverse=True)[0]\n","  top_topic_id = top_topic[0]\n","\n","  entities = []\n","  tk_sents = sent_tokenize(sent)\n","  for ss in tk_sents:\n","    e1, e2 = extract_entities(ss)\n","    entities.append((e1, e2))\n","  \n","  # Add to the current topic as a single entry\n","  topic_wise_entities[top_topic_id].append(entities)\n","\n","topic_entities_json = os.path.join(drive_path, 'kialo_topics', 'topic_entities.json')\n","with open(topic_entities_json, 'w') as f:\n","  json.dump(topic_wise_entities, f)\n","\"\"\""]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663610411509,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"aeO0DnTWy1p8","outputId":"603d836f-f52b-4c16-8e8c-ec86201b8992"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["import sys\n","import os\n","print(os.path.abspath('.'))\n","def generate_bow(input_sentence, aspect):\n","  # Use topic model to find input sentence's topic, get the words and entities matching\n","  # the ones in input sentence and use that BoW txt for inference.\n","\n","  dictionary = gensim.corpora.Dictionary(all_docs)\n","\n","  bow_topic = os.path.join(drive_path, 'PPLM', 'arg_gen', 'bow_topic.txt')\n","  # bow_ent = os.path.join(drive_path, 'PPLM', 'arg_gen', 'bow_ent.txt')\n","\n","  tokens = sentence_to_seq(input_sentence)\n","  corpus = [dictionary.doc2bow(tokens)]\n","  top_topics = (\n","      lda_model.get_document_topics(corpus, minimum_probability=0.0)\n","  )\n","  # Pick top topic for adding entities\n","  top_topic = sorted(top_topics[0], key=lambda x: x[1], reverse=True)[0]\n","  top_topic_id = top_topic[0]\n","\n","  words_l = topic_wise_words[top_topic_id]\n","  # Knowledge graph: g (find all relations with aspect word)\n","  # rel_aspect_l = g[aspect]\n","  # all_words = words_l + rel_aspect_l\n","  # ent_l = topic_wise_entities[top_topic_id]\n","\n","  #with open(bow_topic, 'w') as f:\n","  #  f.write(f\"{aspect}\\n\")\n","  #  for w in words_l[:-1]:\n","  #    f.write(f'{w}\\n')\n","  #  f.write(f'{words_l[-1]}')\n","\n","from subprocess import Popen, PIPE\n","\n","def run_model(\n","    cond_text, grad_len=30, length=50, stepsize=0.01, kl_scale=0.09,\n","    num_samples=5, window_length=10, idx=1\n","):\n","\n","  with open(os.path.join(drive_path, 'PPLM', f'arg_gen_outputs_{idx}.txt'), 'ab') as f:\n","    process = Popen([\n","      'python', 'run_pplm.py', '-B', './arg_gen/bow_topic.txt', '-D', 'generic', '--window_length', f'{window_length}',\n","      '--class_label', '0', '--cond_text', f'{cond_text}', '--grad_length', f'{grad_len}',\n","      '--length', f'{length}', '--gamma', '1.0', '--num_iterations', '5', '--num_samples', f'{num_samples}',\n","      '--stepsize', f'{stepsize}', '--kl_scale', f'{kl_scale}', '--gm_scale', '0.99', '--colorama',\n","      '--sample', '--discrim_weights', '/content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_epoch_8.pt',\n","      '--discrim_meta', '/content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_meta.json',\n","      '--verbosity', 'quiet'\n","    ], stdout=PIPE)\n","    for line in iter(process.stdout.readline, b\"\"):\n","      sys.stdout.write(line)\n","      f.write(line)\n","\n","  #os.system(\n","  #    f\"python run_pplm.py -B ./arg_gen/bow_topic.txt -D generic \\\n","  #     --class_label 0 --cond_text '{cond_text}' --grad_length {grad_len} \\\n","  #     --length {length} --gamma 1.0 --num_iterations 5 --num_samples 5 \\\n","  #     --stepsize {stepsize} --kl_scale {kl_scale} --gm_scale 0.99 --colorama \\\n","  #     --sample --discrim_weights /content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_epoch_8.pt \\\n","  #     --discrim_meta /content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_meta.json --verbosity quiet\"\n","  #)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388,"status":"ok","timestamp":1663610424018,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"zadIyPYxIUHi","outputId":"841a0ebf-5c31-4015-9f94-d814e8dd976e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/PPLM\n"]}],"source":["os.chdir(os.path.join(drive_path, 'PPLM'))\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":98072,"status":"ok","timestamp":1663610525915,"user":{"displayName":"Indranil Ghosh","userId":"16295482860987061688"},"user_tz":-300},"id":"lj4MUL0Nmsvh","outputId":"99eb9c65-2113-41df-eddf-8f75cb23c486"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.7.0\n","  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n","\u001b[K     |████████████████████████████████| 776.7 MB 3.8 kB/s \n","\u001b[?25hCollecting nltk==3.4.5\n","  Downloading nltk-3.4.5.zip (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 45.0 MB/s \n","\u001b[?25hCollecting colorama==0.4.4\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting transformers==3.4.0\n","  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 45.1 MB/s \n","\u001b[?25hCollecting torchtext==0.3.1\n","  Downloading torchtext-0.3.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 794 kB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0-\u003e-r requirements.txt (line 1)) (0.16.0)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0-\u003e-r requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0-\u003e-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5-\u003e-r requirements.txt (line 2)) (1.15.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 54.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (2022.6.2)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (3.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (21.3)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (2.23.0)\n","Collecting tokenizers==0.9.2\n","  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 27.2 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0-\u003e-r requirements.txt (line 4)) (3.17.3)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (3.0.9)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==3.4.0-\u003e-r requirements.txt (line 4)) (1.1.0)\n","Building wheels for collected packages: nltk, sacremoses\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=c14984669d101dfa80e775b30169b14fee8255809f8c26c4117e4728723e43a4\n","  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b00b282ee78df0228cf22c01fccbc80ef01c1b0bb6328647a1719964085be8f9\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built nltk sacremoses\n","Installing collected packages: dataclasses, torch, tokenizers, sentencepiece, sacremoses, transformers, torchtext, nltk, colorama\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\u001b[0m\n","Successfully installed colorama-0.4.4 dataclasses-0.6 nltk-3.4.5 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.9.2 torch-1.7.0 torchtext-0.3.1 transformers-3.4.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","nltk","torch"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"eCaSXUSzIxBr"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/20 [00:00\u003c?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eSensitive social and political topics should not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eSensitive social and political topics should not be discussed in school, says a report published by a British charity on Thursday.\n","\n","The Child Rights Trust (CRT), which provides education advice for charities that provide free legal advice, said the issue of \"sensitive social and political topics\" should\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eSensitive social and political topics should\u001b[31m not\u001b[0m get in the way of an excellent game! I am very pleased with\u001b[31m how\u001b[0m the game has been received and will be playing it on Saturday as soon as I can! I am hoping\u001b[31m that\u001b[0m you will enjoy the game too.\n","\n","I think the main\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eSensitive social and political topics should\u001b[31m not\u001b[0m be subject to censorship\n","\n","\"I don't want to be a martyr for freedom,\" is\u001b[31m how\u001b[0m I feel when I hear these statements. I do\u001b[31m not\u001b[0m want to be an example to other women\u001b[31m that\u001b[0m they should\u001b[31m not\u001b[0m be afraid, and I do\u001b[31m not\u001b[0m\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eSensitive social and political topics should\u001b[31m not\u001b[0m be taboo.\n","\n","The issue of rape is\u001b[31m not\u001b[0m a taboo in Pakistan. A recent study on the rape of women and girls by police has shown\u001b[31m this\u001b[0m, but\u001b[31m that\u001b[0m was only in one\u001b[31m case\u001b[0m where it was a rape in the police\u001b[31m case\u001b[0m. So\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r  5%|5         | 1/20 [07:56\u003c2:30:57, 476.73s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003esearching should not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003esearching should not be done in the context of \"what to expect\", \"what will be good\", \"what's good\" or the like. It must always be \"what will make you want to do it\".\n","\n","\n","This is especially true for searches where you\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003esearching should\u001b[31m not\u001b[0m result in a return of an empty result\n","\n","searching should\u001b[31m not\u001b[0m result in the return code of a function\u001b[31m not\u001b[0m found\n","\n","Search for elements of certain types (if\u001b[31m any\u001b[0m) in\u001b[31m any\u001b[0m element type in the DOM\n","\n","if (typeof (\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003esearching should\u001b[31m not\u001b[0m take\u001b[31m any\u001b[0m more effort than searching with an actual keyword.\n","\n","searching should\u001b[31m not\u001b[0m\u001b[31m need\u001b[0m to know\u001b[31m that\u001b[0m a particular keyword, for example an e-mail domain, has an exact match with another domain\n","\n","searching should\u001b[31m not\u001b[0m ask a search engine\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003esearching should\u001b[31m not\u001b[0m only mean searching the Internet; it should also mean\u001b[31m that\u001b[0m a search on a particular word should be limited to the word which is actually searched, i.e., \"dude\" and\u001b[31m not\u001b[0m, say, \"dude\".\n","\n","The\u001b[31m reason\u001b[0m for\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 10%|#         | 2/20 [15:08\u003c2:15:03, 450.18s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eOrgan donation should not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eOrgan donation should not be the sole purpose of life saving medical procedures, as it is for some procedures that are performed at hospitals.\n","\n","Organ donation should only occur in the cases where the donor wishes to remain anonymous. In this way, the donor's identity remains unknown\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eOrgan donation should\u001b[31m not\u001b[0m be used as a means to fund political campaigns.\n","\n","I am a member of The Church Of Jesus Christ of Latter-Day Saints, and I don't think I've ever seen such a statement in the history of politics, or of American politics\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eOrgan donation should\u001b[31m not\u001b[0m be a barrier to getting your child an early childhood education\n","\n","In my view, the best way\u001b[31m that\u001b[0m we can provide\u001b[31m that\u001b[0m is to provide a child of\u001b[31m any\u001b[0m age\u001b[31m that\u001b[0m has\u001b[31m not\u001b[0m yet received\u001b[31m that\u001b[0m early childhood education with the tools, the resources, to\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eOrgan donation should\u001b[31m not\u001b[0m only protect people who have lost a loved one, but should also help save the lives of children, who are often at risk of death\u001b[31m because\u001b[0m of poor treatment, says Peter Parnis, professor of epidemiology at the University of Oxford, who has\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 15%|#5        | 3/20 [22:40\u003c2:07:51, 451.29s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eWorld should not adopt\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eWorld should not adopt'sustainable' carbon-pricing scheme\n","\n","World Bank: 'Sustainable' carbon-pricing scheme is 'not sustainable'\n","\n","Sierra Club to host Climate Justice Summit on March 22 in New Orleans\n","\n","'The world must\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eWorld should\u001b[31m not\u001b[0m adopt a \"new\" approach to global climate\u001b[31m change\u001b[0m.\n","\n","I am talking about the current \"new\" approach to climate\u001b[31m change\u001b[0m which is one of \"dramatic reductions in greenhouse gases\u001b[31m that\u001b[0m are\u001b[31m not\u001b[0m compatible with the future of life as we know it\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eWorld should\u001b[31m not\u001b[0m adopt \"one nation under God,\" says former president\n","\n","\"I think the United Nations should be no more than the United States,\" he added. He said\u001b[31m that\u001b[0m \"this world was created by our own hand.\"\n","\n","\"We should\u001b[31m not\u001b[0m be in\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eWorld should\u001b[31m not\u001b[0m adopt the approach of the US and the EU\u001b[31m that\u001b[0m it will have to accept a 'one-size-fits-all' solution to\u001b[31m any\u001b[0m issue, says Russian foreign minister Lavrov.\n","\n","© REUTERS / Alexei Druzhinin The New Cold War\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 20%|##        | 4/20 [30:21\u003c2:01:20, 455.05s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eGovernments all around the world should not push push\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eGovernments all around the world should not push push their own interests in an increasingly fragmented world in favour of the interests of one group of individuals. It is time for the world to move beyond the current economic models that seek to maximize wealth and privilege the wealthy at the expense of the majority of humanity.\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eGovernments all around the world should\u001b[31m not\u001b[0m push push their economies towards zero and instead focus on creating jobs instead. In my view the\u001b[31m reason\u001b[0m\u001b[31m that\u001b[0m governments have\u001b[31m not\u001b[0m done so for a long time is\u001b[31m because\u001b[0m of the cost of doing business and the\u001b[31m fact\u001b[0m\u001b[31m that\u001b[0m they have\u001b[31m not\u001b[0m invested in new jobs or infrastructure. In\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eGovernments all around the world should\u001b[31m not\u001b[0m push push ahead with nuclear power\u001b[31m because\u001b[0m nuclear power will never work. But they can.\n","\n","The first step in\u001b[31m any\u001b[0m such project,\u001b[31m if\u001b[0m it is to succeed, should be to have it built properly. This is a basic prerequisite.\n","\n","A good deal\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eGovernments all around the world should\u001b[31m not\u001b[0m push push the boundaries of the Internet by creating new technologies, but should rather use existing technologies to create a global infrastructure\u001b[31m that\u001b[0m would allow countries like ours to continue to innovate, create jobs, and grow our economy. This is\u001b[31m why\u001b[0m we are launching the Global Citizen Challenge\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 25%|##5       | 5/20 [38:11\u003c1:55:06, 460.44s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eThe progression of AI and tech in the legal industry is not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eThe progression of AI and tech in the legal industry is not just about creating machines that understand us better than we understand ourselves. It is about creating machines that can take a case to trial. That is what has been done in the AI courtroom.\n","\n","The court has been called the \"death penalty's court\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eThe progression of AI and tech in the legal industry is\u001b[31m not\u001b[0m without controversy and challenges. In recent years AI and tech has been used in the legal industry in many cases. In a recent\u001b[31m case\u001b[0m, the judge of\u001b[31m that\u001b[0m\u001b[31m case\u001b[0m was a prominent attorney, who had a lot of experience in\u001b[31m this\u001b[0m field and he was a\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eThe progression of AI and tech in the legal industry is\u001b[31m not\u001b[0m only accelerating, but also is changing our view of\u001b[31m how\u001b[0m we should think about what it means to be in a\u001b[31m job\u001b[0m.\n","\n","I'm\u001b[31m not\u001b[0m saying\u001b[31m that\u001b[0m you should be doing\u001b[31m this\u001b[0m\u001b[31m job\u001b[0m, you can do other things, but I do think there will\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eThe progression of AI and tech in the legal industry is\u001b[31m not\u001b[0m only driven by the desire to improve legal performance; we are also seeing a shift towards an understanding of what AI\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m achieve. For example, AI is\u001b[31m not\u001b[0m able to do a simple calculation, or recognize a legal term in its context, without a\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 30%|###       | 6/20 [46:09\u003c1:48:48, 466.31s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eFeminism does not empower\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eFeminism does not empower men to abuse women, but to do it to each other. In fact, feminism is designed to empower women to abuse each other. And as a consequence, men are forced to abuse each other.\n","\n","This is the core argument of The Myth\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eFeminism\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m empower women to achieve their full potential and equality in\u001b[31m any\u001b[0m way is\u001b[31m not\u001b[0m about equality in\u001b[31m any\u001b[0m way whatsoever.\n","\n","The feminist agenda\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m include equality for all women or anyone in\u001b[31m any\u001b[0m way\u001b[31m that\u001b[0m they are\u001b[31m not\u001b[0m women. The only\u001b[31m reason\u001b[0m they call themselves a\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eFeminism\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m empower women.\n","\n","This is a\u001b[31m fact\u001b[0m,\u001b[31m not\u001b[0m a\u001b[31m fact\u001b[0moid,\u001b[31m not\u001b[0m a joke. If you are a feminist and you think \"I know,\u001b[31m why\u001b[0m can't women do it?\"\u001b[31m this\u001b[0m is the answer you will find.\n","\n","I know\u001b[31m this\u001b[0m\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eFeminism\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m empower women. We have to be feminists\u001b[31m because\u001b[0m it is what we believe to be best.\n","\n","This is\u001b[31m why\u001b[0m I am\u001b[31m not\u001b[0m a feminist\u001b[31m because\u001b[0m I do\u001b[31m not\u001b[0m believe\u001b[31m that\u001b[0m women should have to choose between being empowered and being safe.\n","\n","I am a\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 35%|###5      | 7/20 [53:43\u003c1:40:11, 462.44s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eAstrology is not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eAstrology is not science, but it is an approach to observing how the heavens work. This is what astronomers do, and this is what we do with the data we gather.\n","\n","Astrology is not a scientific approach, but it is an approach to observing how\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eAstrology is\u001b[31m not\u001b[0m just about stars or planets orbiting the sun. Astronomy has to be about the interactions between the physical laws of the universe, which is\u001b[31m how\u001b[0m we know\u001b[31m that\u001b[0m we are here in\u001b[31m this\u001b[0m universe at all. The laws\u001b[31m that\u001b[0m are in play have\u001b[31m not\u001b[0m changed much\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eAstrology is\u001b[31m not\u001b[0m science.\n","\n","This is\u001b[31m not\u001b[0m an argument\u001b[31m that\u001b[0m can be made,\u001b[31m because\u001b[0m science is\u001b[31m not\u001b[0m science. It's\u001b[31m not\u001b[0m an argument\u001b[31m that\u001b[0m can be made,\u001b[31m because\u001b[0m science doesn't work like\u001b[31m that\u001b[0m. Science can only work when you believe, and there is no\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eAstrology is\u001b[31m not\u001b[0m only a science\u001b[31m that\u001b[0m deals with the stars but it can also have its place in modern society. Astrology has many benefits and is one of the most effective methods\u001b[31m that\u001b[0m are available to help people in dealing with life.\n","\n","It is a great source\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 40%|####      | 8/20 [1:01:14\u003c1:31:44, 458.70s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eAll our actions are not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eAll our actions are not just about money. They are about your health, your safety, and your future as a citizen of Australia.\n","\n","If you want to know how you can support us and get involved, please click here for our information and links section.\n","\n","\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eAll our actions are\u001b[31m not\u001b[0m just based on our minds.\n","\n","In our daily lives, we act in the following ways:\n","\n","• Thinking\n","\n","We must learn to be more aware of the things\u001b[31m that\u001b[0m influence our thinking in the future.\n","\n","• Action\n","\n","\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eAll our actions are\u001b[31m not\u001b[0m only in our heads but also in our hands. If we don't make our decisions consciously then\u001b[31m how\u001b[0m can we ever learn from these choices? We should\u001b[31m not\u001b[0m be afraid of doing what is necessary. We have to choose and choose, and choose what will\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eAll our actions are\u001b[31m not\u001b[0m only good.\n","\n","The only thing\u001b[31m that\u001b[0m is bad about our world is\u001b[31m that\u001b[0m there is so much\u001b[31m that\u001b[0m we do\u001b[31m not\u001b[0m want to do, or do\u001b[31m not\u001b[0m know\u001b[31m how\u001b[0m to do. In\u001b[31m this\u001b[0m\u001b[31m case\u001b[0m, it is\u001b[31m because\u001b[0m we are too lazy to try out\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 45%|####5     | 9/20 [1:08:47\u003c1:23:44, 456.79s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eJournalists not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eJournalists not affiliated with the news service have been instructed in how to conduct interviews on the streets of Paris.\n","\n","The instructions to reporters were issued by the French interior minister, Manuel Valls.\n","\n","The rules for the press are strict and the rules are\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eJournalists\u001b[31m not\u001b[0m only cover and write about the news of the day, but also provide information to the public on the issues which most affect them.\n","\n","This is\u001b[31m because\u001b[0m we are a profession and a profession should\u001b[31m not\u001b[0m be treated like an afterthought or an afterthought\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eJournalists\u001b[31m not\u001b[0m only want access to the most important data, but they also want to make sure\u001b[31m that\u001b[0m the data they are publishing is as accurate as\u001b[31m possible\u001b[0m. We do\u001b[31m not\u001b[0m publish raw data for our analysis of the relationship between education and earnings.\n","\n","A recent analysis\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eJournalists\u001b[31m not\u001b[0m only report on the news; they also report it well.\n","\n","The Washington Post recently published a series of stories about the growing number of women in the military who are leaving the military after their service. But the stories are\u001b[31m not\u001b[0m the news.\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 50%|#####     | 10/20 [1:16:14\u003c1:15:39, 453.91s/it]"]},{"name":"stdout","output_type":"stream","text":["= Prefix of sentence =\n","\u003c|endoftext|\u003eConscientious objection to abortion should not\n","\n","= Unperturbed generated text =\n","\u003c|endoftext|\u003eConscientious objection to abortion should not be defined as'religious' but rather as a'moral objection': it is 'not religious at all'.\n","\n","The new guidelines, which come into force from 1 January next year, will allow abortion providers to choose to provide the procedure if they\n","\n","= Perturbed generated text 1 =\n","\u003c|endoftext|\u003eConscientious objection to abortion should\u001b[31m not\u001b[0m be based on religious belief, and should\u001b[31m not\u001b[0m be treated differently to\u001b[31m any\u001b[0m other conscientious objection.\n","\n","In\u001b[31m this\u001b[0m\u001b[31m case\u001b[0m, the judge is correct in stating\u001b[31m that\u001b[0m the law is\u001b[31m not\u001b[0m clear,\u001b[31m that\u001b[0m there\u001b[31m could\u001b[0m be a legitimate\u001b[31m reason\u001b[0m\u001b[31m not\u001b[0m to have an abortion,\n","\n","= Perturbed generated text 2 =\n","\u003c|endoftext|\u003eConscientious objection to abortion should\u001b[31m not\u001b[0m be treated as an individual\u001b[31m matter\u001b[0m, as the courts have said, and therefore\u001b[31m that\u001b[0m is\u001b[31m not\u001b[0m the\u001b[31m case\u001b[0m in\u001b[31m this\u001b[0m\u001b[31m case\u001b[0m. I think\u001b[31m that\u001b[0m it is, and\u001b[31m that\u001b[0m we\u001b[31m need\u001b[0m to take\u001b[31m that\u001b[0m seriously. I also believe\u001b[31m that\u001b[0m, even\u001b[31m if\u001b[0m we don't agree\n","\n","= Perturbed generated text 3 =\n","\u003c|endoftext|\u003eConscientious objection to abortion should\u001b[31m not\u001b[0m mean\u001b[31m that\u001b[0m women who decide\u001b[31m not\u001b[0m to have one should\u001b[31m not\u001b[0m have access to a legal abortion at all, the Supreme Court of Canada has ruled. In a 4-3 vote, the Court of Appeal said\u001b[31m that\u001b[0m a woman who\u001b[31m does\u001b[0m\u001b[31m not\u001b[0m wish to become pregnant\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r 55%|#####5    | 11/20 [1:23:50\u003c1:08:10, 454.47s/it]"]}],"source":["i_sentences = [\n","    \"Sensitive social and political topics should be discussed in school\",\n","    \"It's better to search bitter\",\n","    \"Organ donation should be mandatory\",\n","    \"World should adopt International Fixed Calendar\",\n","    \"Governments all around the world should push for 100% renewable energy as fast as possible\",\n","    \"The progression of AI and tech in the legal industry is useful for all parties\",\n","    \"Feminism empowers both women and men\",\n","    \"Astrology is a valid practice\",\n","    \"All our actions are selfish to some degree\",\n","    \"Journalists have a moral obligation to display violent images\",\n","    \"Conscientious objection to abortion should be banned\",\n","    \"We should spend our spare time helping others rather than in self-development\",\n","    \"Protecting individual data privacy is necessary for a healthy society.\",\n","    \"Politicians should spend a minimal amount of hours each year in charitable activities.\",\n","    \"Employees Should Disclose Their Mental Health Conditions In The Workplace\",\n","    \"Quantum physics is not for us to know the future\",\n","    \"Angels and demons exist in objective reality.\",\n","    \"It is impossible to say whether angels and demons exist.\",\n","    \"Election campaigns should only be funded by the government\",\n","    \"YouTube is enforcing censorship through the demonetization policy.\"\n","]\n","\n","cond_text = [\n","    \"Sensitive social and political topics should not\",\n","    \"searching should not\",\n","    \"Organ donation should not\",\n","    \"World should not adopt\",\n","    \"Governments all around the world should not push push\",\n","    \"The progression of AI and tech in the legal industry is not\",\n","    \"Feminism does not empower\",\n","    \"Astrology is not\",\n","    \"All our actions are not\",\n","    \"Journalists not\",\n","    \"Conscientious objection to abortion should not\",\n","    \"We should spend our spare time not to\",\n","    \"Protecting individual data privacy is no\",\n","    \"Politicians should not\",\n","    \"Employees Should not\",\n","    \"Quantum physics is\",\n","    \"Angels and demons are not\",\n","    \"angels and demons exist.\",\n","    \"Election campaigns should not\",\n","    \"YouTube is not\"\n","\n","\n","]\n","\n","aspects = [\n","    \"school\",\n","    \"bitter\",\n","    \"save lives\",\n","    \"fixed\",\n","    \"renewable energy\",\n","    \"revolution\",\n","    \"social development\",\n","    \"space initiative\",\n","    \"self development\",\n","    \"information\",\n","    \"failure\",\n","    \"livelihood\",\n","    \"safe\",\n","    \"economic development\",\n","    \"safe workplace\",\n","    \"future\",\n","    \"real\",\n","    \"omniscious\",\n","    \"democracy\",\n","    \"censor board\"\n","]\n","\n","idx = 1\n","num_samples = 3\n","for i in tqdm(range(len(cond_text)), ascii=True):\n","  inp = i_sentences[i]\n","  cond = cond_text[i]\n","  aspect = aspects[i]\n","  \n","  generate_bow(inp, aspect=aspect)\n","  # Will generate num_samples perturbed samples for each input triplet\n","  # (inp, cond, aspect). Saved to ./arg_gen_outputs_{idx}.txt\n","  with open(os.path.join(drive_path, 'PPLM', f'arg_gen_outputs_{idx}.txt'), 'w') as f:\n","    f.write(f\"Input: {inp}\\n\")\n","    f.write(f\"Conditional text: {cond}\\n\")\n","    f.write(f\"Aspect: {aspect}\\n\")\n","  run_model(cond, num_samples=num_samples, idx=idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFUASdeQ6jbF"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNswCzcQCicpX39ujsyi+xn","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1hfuub6nfxGVkdR-cvnAn--P6ux4DS71r","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}