{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28978,"status":"ok","timestamp":1666862982559,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"},"user_tz":-360},"id":"BWEtlnYlsdY6","outputId":"2cadd757-1653-4202-8b54-54b09fe0f630"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycld2\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[K     |████████████████████████████████| 41.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Building wheels for collected packages: pycld2\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834370 sha256=909099cb6acd9bc42f42e9d45fb76af2b0587392c0d1f83176778226b5622c26\n","  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n","Successfully built pycld2\n","Installing collected packages: pycld2\n","Successfully installed pycld2-0.41\n"]}],"source":["!pip install pycld2 regex nltk gensim spacy"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-YuuBVgNzmF","executionInfo":{"status":"ok","timestamp":1666863008031,"user_tz":-360,"elapsed":25480,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"}},"outputId":"ccf51e38-834c-4ee5-8c5a-b20f8963ccf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15954,"status":"ok","timestamp":1666863023980,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"},"user_tz":-360},"id":"5RWoVY6UsHgh","outputId":"8a8bceae-f447-4d08-aaba-2656a433289e"},"outputs":[{"output_type":"stream","name":"stdout","text":["5% of sentences done\n","10% of sentences done\n","15% of sentences done\n","20% of sentences done\n","25% of sentences done\n","30% of sentences done\n","35% of sentences done\n","40% of sentences done\n","45% of sentences done\n","50% of sentences done\n","55% of sentences done\n","60% of sentences done\n","65% of sentences done\n","70% of sentences done\n","75% of sentences done\n","80% of sentences done\n","85% of sentences done\n","90% of sentences done\n","95% of sentences done\n","100% of sentences done\n"]}],"source":["import json\n","import pycld2 as cld2\n","\n","with open('/content/drive/MyDrive/kialo_corpus.json', 'r') as f:\n","    out = json.load(f)\n","\n","# Need to use regex to remove offending non-UTF-8 characters from the data, which\n","# causes (error: input contains invalid UTF-8 around byte ...)\n","# Ref: https://github.com/aboSamoor/polyglot/issues/71#issuecomment-707997790\n","\n","import regex\n","import math\n","\n","RE_BAD_CHARS = regex.compile(r\"[\\p{Cc}\\p{Cs}]+\")\n","\n","def remove_bad_chars(text):\n","    return RE_BAD_CHARS.sub(\"\", text)\n","\n","def detect_en(text):\n","    _, _, _, detection = cld2.detect(text, returnVectors=True)\n","    for tup in detection:\n","        if 'en' not in tup[-1]:\n","            return False\n","    return True\n","\n","out_filtered = []\n","prev_num = 0\n","for i in range(len(out)):\n","    x = out[i]\n","    try:\n","        x['text'] = remove_bad_chars(x['text'])\n","        if detect_en(x['text']):\n","            out_filtered.append(x)\n","    except Exception as e:\n","        print(x['text'])\n","        print(f'Exception {e} raised')\n","        break\n","    percent = (i+1) / len(out) * 100\n","    _, num = math.modf(percent)\n","    num_ = int(num - (num % 5))\n","    if num_ != prev_num:\n","        for x in range(prev_num+5, num_+1, 5):\n","            print(f\"{x}% of sentences done\")\n","        prev_num = num_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3410,"status":"ok","timestamp":1666863027387,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"},"user_tz":-360},"id":"AjG8_f4Ir5SR","outputId":"92ae794f-c104-438a-941d-009cddb801b8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["import nltk\n","from nltk.stem import SnowballStemmer, WordNetLemmatizer\n","import re\n","import os\n","import pickle\n","import gensim\n","from tqdm import tqdm\n","\n","nltk.download(\"wordnet\")\n","nltk.download(\"omw-1.4\")\n","\n","stemmer = SnowballStemmer(\"english\")\n","\n","models = {}\n","word_map = {}\n","debug = False\n","drive_path = '/content/drive/MyDrive'\n","#drive_path = './'\n","\n","num_topics = 50\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","# Tokenize and lemmatize\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text) :\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","            \n","    return result\n","\n","def sentence_to_seq(text):\n","    split_words = set(text.split())\n","    tokens = preprocess(re.sub(r'http\\S+', '', text))\n","\n","    # Keep a mapping of stems to original words\n","    if not os.path.exists('word_map.pkl'):\n","        for tk in tokens:\n","            for word in split_words:\n","                if tk in word:\n","                    if not word_map.get(tk):\n","                        word_map[tk] = set()\n","                    word_map[tk].add(word)\n","    \n","    return tokens\n","\n","def prep_docs(out_filtered):\n","  all_docs = []\n","  all_sents = []\n","  for i, x in enumerate(tqdm(out_filtered, ascii=True)):\n","    topic_id, _ = x['id'].strip().split('.')\n","    if not models.get(topic_id):\n","      models[topic_id] = {}\n","    if x['neutral']:\n","      all_sents.append(x['neutral'][0]['text'])\n","      seq = sentence_to_seq(x['neutral'][0]['text'])\n","      models[topic_id]['topic'] = seq\n","      all_docs.append(seq)\n","\n","    for obj in x['pro']:\n","      if not models[topic_id].get('pro'):\n","        models[topic_id]['pro'] = []\n","      all_sents.append(obj['text'])\n","      seq = sentence_to_seq(obj['text'])\n","      models[topic_id]['pro'].append(seq)\n","      all_docs.append(seq)\n","    \n","    for obj in x['con']:\n","      if not models[topic_id].get('con'):\n","        models[topic_id]['con'] = []\n","      all_sents.append(obj['text'])\n","      seq = sentence_to_seq(obj['text'])\n","      models[topic_id]['con'].append(seq)\n","      all_docs.append(seq)\n","\n","  return all_docs, all_sents\n","\n","def model_topics(processed_docs, num_topics=10):\n","    os.makedirs(os.path.join(drive_path, 'kialo_topics'), exist_ok=True)\n","    model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","    if os.path.exists(model_path):\n","        lda_model = gensim.models.LdaMulticore.load(model_path)\n","    else:\n","        dictionary = gensim.corpora.Dictionary(processed_docs)\n","        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","        lda_model =  gensim.models.LdaMulticore(\n","            bow_corpus, num_topics = num_topics,\n","            id2word = dictionary, passes = 10, workers = 8\n","        )\n","        # Save the model\n","        lda_model.save(model_path)\n","\n","def model_subtopics(processed_docs, topic_id, stance):\n","    fname = f'models/lda_topics_{topic_id}_{stance}.ckpt'\n","    if os.path.exists(fname):\n","        lda_model = gensim.models.LdaMulticore.load(fname)\n","    else:\n","        dictionary = gensim.corpora.Dictionary(processed_docs)\n","        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","        lda_model =  gensim.models.LdaMulticore(\n","            bow_corpus, num_topics = 10, id2word = dictionary, passes = 20, workers = 4\n","        )\n","        # Save the model\n","        lda_model.save(fname)\n","    \n","    if debug:\n","      # Print each topic discovered with it's top-40 words (tokens)\n","      for idx, topic in lda_model.print_topics(num_words=5):\n","          topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","          topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","          topic_words = [min(x, key=len) if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)]\n","          print(f\"For topic ID {topic_id} and stance {stance}\")\n","          print(\"Topic: {} => Words: {}\".format(idx, ','.join(topic_words)))\n","    return fname\n","\n","def run_topic_modeling():\n","  global models, word_map, debug, num_topics\n","  all_docs = []\n","  all_sents = []\n","  topic_model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","  if not os.path.exists(topic_model_path):\n","    all_docs, all_sents = prep_docs(out_filtered)\n","    model_topics(all_docs, num_topics=num_topics)\n","  else:\n","    pass\n","    #all_docs, all_sents = prep_docs(out_filtered)\n","\n","  if not os.path.exists(os.path.join(drive_path, 'word_map.pkl')):\n","    with open(os.path.join(drive_path, 'word_map.pkl'), 'wb') as f:\n","      pickle.dump(word_map, f)\n","  else:\n","    with open(os.path.join(drive_path, 'word_map.pkl'), 'rb') as f:\n","      word_map = pickle.load(f)\n","\n","  return all_docs, all_sents\n","\n","all_docs, all_sents = run_topic_modeling()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103124,"status":"ok","timestamp":1666863130498,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"},"user_tz":-360},"id":"eZDO0XsBQs9s","outputId":"0e6dd834-7723-4351-8497-80acda7e974f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-lg==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n","\u001b[K     |████████████████████████████████| 587.7 MB 17 kB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.1) (3.4.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.6.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.4.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["!python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":10622,"status":"ok","timestamp":1666863141113,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"},"user_tz":-360},"id":"D_kPMsJ6SFnp","outputId":"4000be9f-4653-40dd-8546-ac54c25efe43"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Topic 0: 0.059*\"relationship\" + 0.044*\"medic\" + 0.032*\"treat\" + 0.032*\"respect\" + 0.030*\"ethic\" + 0.029*\"treatment\" + 0.025*\"doctor\" + 0.022*\"patient\" + 0.017*\"coupl\" + 0.017*\"peopl\" + 0.017*\"profession\" + 0.016*\"theft\" + 0.015*\"procedur\" + 0.013*\"homosexu\" + 0.012*\"steal\" + 0.012*\"warn\" + 0.011*\"virtu\" + 0.011*\"person\" + 0.011*\"enemi\" + 0.010*\"surgeri\" + 0.009*\"investig\" + 0.009*\"digniti\" + 0.008*\"chines\" + 0.008*\"hospit\" + 0.008*\"secret\" + 0.008*\"privaci\" + 0.007*\"confus\" + 0.007*\"perform\" + 0.007*\"cheat\" + 0.006*\"invas\" + 0.006*\"request\" + 0.006*\"donald\" + 0.006*\"kong\" + 0.006*\"hong\" + 0.006*\"starv\" + 0.006*\"undergo\" + 0.006*\"condemn\" + 0.005*\"euthanasia\" + 0.005*\"washington\" + 0.005*\"want\"\n","Topic 1: 0.177*\"caus\" + 0.122*\"harm\" + 0.030*\"justifi\" + 0.027*\"protest\" + 0.023*\"violent\" + 0.023*\"donat\" + 0.018*\"effect\" + 0.017*\"chariti\" + 0.015*\"peopl\" + 0.015*\"trigger\" + 0.014*\"trait\" + 0.013*\"psycholog\" + 0.011*\"carbon\" + 0.010*\"counter\" + 0.010*\"broad\" + 0.009*\"regim\" + 0.008*\"root\" + 0.008*\"individu\" + 0.007*\"spectrum\" + 0.007*\"organ\" + 0.007*\"prevent\" + 0.007*\"deliv\" + 0.006*\"confin\" + 0.006*\"capita\" + 0.006*\"extrem\" + 0.005*\"allevi\" + 0.005*\"degrad\" + 0.005*\"societi\" + 0.005*\"help\" + 0.005*\"inferior\" + 0.005*\"lead\" + 0.005*\"shut\" + 0.005*\"saudi\" + 0.005*\"danger\" + 0.005*\"tampon\" + 0.004*\"advers\" + 0.004*\"hitler\" + 0.004*\"ptsd\" + 0.004*\"fascist\" + 0.004*\"physic\"\n","Topic 2: 0.130*\"exist\" + 0.068*\"univers\" + 0.021*\"physic\" + 0.017*\"begin\" + 0.015*\"conscious\" + 0.014*\"imposs\" + 0.014*\"perfect\" + 0.014*\"model\" + 0.013*\"mind\" + 0.013*\"observ\" + 0.013*\"possibl\" + 0.013*\"concept\" + 0.013*\"natur\" + 0.012*\"predict\" + 0.011*\"time\" + 0.011*\"creat\" + 0.010*\"entiti\" + 0.009*\"infinit\" + 0.009*\"world\" + 0.008*\"classic\" + 0.008*\"explan\" + 0.007*\"machin\" + 0.007*\"mechan\" + 0.007*\"life\" + 0.007*\"requir\" + 0.007*\"event\" + 0.007*\"defin\" + 0.006*\"reason\" + 0.006*\"random\" + 0.006*\"like\" + 0.006*\"theori\" + 0.006*\"contradict\" + 0.005*\"mean\" + 0.005*\"creator\" + 0.005*\"simul\" + 0.005*\"matter\" + 0.005*\"understand\" + 0.004*\"definit\" + 0.004*\"explain\" + 0.004*\"actual\"\n","Topic 3: 0.051*\"motiv\" + 0.042*\"slaveri\" + 0.037*\"slave\" + 0.024*\"reward\" + 0.022*\"push\" + 0.022*\"survey\" + 0.021*\"margin\" + 0.020*\"deserv\" + 0.019*\"resist\" + 0.019*\"pursu\" + 0.017*\"respond\" + 0.017*\"peopl\" + 0.015*\"impeach\" + 0.014*\"recipi\" + 0.013*\"worri\" + 0.013*\"forward\" + 0.012*\"injustic\" + 0.011*\"escap\" + 0.010*\"ineffect\" + 0.009*\"binari\" + 0.009*\"korean\" + 0.008*\"realiz\" + 0.008*\"communism\" + 0.007*\"confront\" + 0.007*\"cure\" + 0.007*\"sham\" + 0.007*\"haven\" + 0.007*\"beg\" + 0.006*\"surveil\" + 0.006*\"flag\" + 0.006*\"suspect\" + 0.006*\"agenda\" + 0.006*\"introduct\" + 0.005*\"fascism\" + 0.005*\"resent\" + 0.005*\"unrel\" + 0.005*\"invit\" + 0.005*\"putin\" + 0.005*\"come\" + 0.005*\"seven\"\n","Topic 4: 0.065*\"area\" + 0.055*\"drive\" + 0.055*\"class\" + 0.038*\"surviv\" + 0.027*\"car\" + 0.024*\"fuel\" + 0.018*\"middl\" + 0.015*\"obes\" + 0.014*\"road\" + 0.013*\"divis\" + 0.013*\"safer\" + 0.013*\"credit\" + 0.012*\"fossil\" + 0.012*\"superior\" + 0.011*\"phone\" + 0.010*\"card\" + 0.010*\"aim\" + 0.010*\"magic\" + 0.009*\"like\" + 0.009*\"rural\" + 0.009*\"prayer\" + 0.008*\"elit\" + 0.008*\"wizard\" + 0.008*\"world\" + 0.008*\"urban\" + 0.007*\"miracl\" + 0.007*\"notic\" + 0.007*\"name\" + 0.007*\"instinct\" + 0.007*\"self\" + 0.007*\"worthi\" + 0.006*\"defeat\" + 0.006*\"reproduc\" + 0.006*\"insect\" + 0.006*\"fish\" + 0.006*\"smart\" + 0.006*\"criticis\" + 0.006*\"algorithm\" + 0.006*\"quantiti\" + 0.006*\"assur\"\n","Topic 5: 0.054*\"technolog\" + 0.047*\"weapon\" + 0.038*\"nuclear\" + 0.030*\"energi\" + 0.028*\"advanc\" + 0.017*\"job\" + 0.015*\"autom\" + 0.015*\"war\" + 0.015*\"light\" + 0.014*\"destroy\" + 0.014*\"effici\" + 0.013*\"currenc\" + 0.012*\"star\" + 0.012*\"develop\" + 0.010*\"movi\" + 0.009*\"power\" + 0.009*\"creat\" + 0.009*\"speed\" + 0.008*\"slow\" + 0.008*\"renew\" + 0.008*\"convent\" + 0.007*\"capabl\" + 0.007*\"util\" + 0.007*\"film\" + 0.007*\"ship\" + 0.007*\"disrupt\" + 0.007*\"strike\" + 0.006*\"ineffici\" + 0.006*\"dark\" + 0.006*\"solar\" + 0.006*\"sourc\" + 0.006*\"wind\" + 0.006*\"pretti\" + 0.005*\"serious\" + 0.005*\"bomb\" + 0.005*\"plant\" + 0.005*\"incid\" + 0.005*\"vice\" + 0.005*\"strateg\" + 0.005*\"produc\"\n","Topic 6: 0.123*\"moral\" + 0.036*\"wrong\" + 0.034*\"scienc\" + 0.034*\"scientif\" + 0.030*\"test\" + 0.024*\"correct\" + 0.024*\"method\" + 0.021*\"genet\" + 0.018*\"theori\" + 0.011*\"immor\" + 0.011*\"scientist\" + 0.010*\"oblig\" + 0.010*\"gorilla\" + 0.010*\"ethic\" + 0.010*\"code\" + 0.008*\"person\" + 0.008*\"philosoph\" + 0.007*\"modifi\" + 0.007*\"framework\" + 0.007*\"base\" + 0.007*\"understand\" + 0.007*\"error\" + 0.006*\"gene\" + 0.006*\"cod\" + 0.006*\"monarch\" + 0.006*\"consid\" + 0.006*\"determin\" + 0.006*\"accept\" + 0.006*\"question\" + 0.006*\"hypothesi\" + 0.006*\"standard\" + 0.005*\"flaw\" + 0.005*\"exampl\" + 0.005*\"pull\" + 0.005*\"differ\" + 0.005*\"societi\" + 0.005*\"mean\" + 0.005*\"individu\" + 0.004*\"fact\" + 0.004*\"consensus\"\n","Topic 7: 0.104*\"leav\" + 0.040*\"target\" + 0.034*\"movement\" + 0.031*\"suicid\" + 0.028*\"organis\" + 0.023*\"agenc\" + 0.022*\"accus\" + 0.017*\"peopl\" + 0.017*\"identifi\" + 0.015*\"news\" + 0.015*\"nippl\" + 0.014*\"join\" + 0.014*\"licens\" + 0.013*\"facebook\" + 0.012*\"breast\" + 0.011*\"like\" + 0.011*\"devic\" + 0.011*\"persecut\" + 0.011*\"lethal\" + 0.010*\"everybodi\" + 0.010*\"driver\" + 0.010*\"attempt\" + 0.010*\"feasibl\" + 0.009*\"street\" + 0.009*\"isi\" + 0.008*\"stick\" + 0.008*\"mislead\" + 0.008*\"twitter\" + 0.007*\"ritual\" + 0.007*\"charit\" + 0.007*\"particl\" + 0.007*\"fake\" + 0.006*\"appl\" + 0.006*\"patern\" + 0.006*\"homeless\" + 0.005*\"account\" + 0.005*\"includ\" + 0.005*\"trace\" + 0.005*\"elig\" + 0.004*\"march\"\n","Topic 8: 0.058*\"choos\" + 0.054*\"feel\" + 0.050*\"peopl\" + 0.045*\"abus\" + 0.043*\"suffer\" + 0.031*\"consequ\" + 0.027*\"person\" + 0.024*\"pain\" + 0.023*\"fear\" + 0.022*\"emot\" + 0.019*\"consent\" + 0.016*\"victim\" + 0.015*\"like\" + 0.011*\"execut\" + 0.011*\"deni\" + 0.010*\"negat\" + 0.009*\"russian\" + 0.009*\"hurt\" + 0.008*\"comfort\" + 0.008*\"individu\" + 0.008*\"sexual\" + 0.008*\"rape\" + 0.007*\"situat\" + 0.007*\"want\" + 0.006*\"life\" + 0.006*\"live\" + 0.006*\"pleasur\" + 0.006*\"bond\" + 0.005*\"lead\" + 0.005*\"deliber\" + 0.005*\"guilti\" + 0.005*\"make\" + 0.005*\"mean\" + 0.005*\"death\" + 0.005*\"reason\" + 0.005*\"inflict\" + 0.005*\"result\" + 0.005*\"physic\" + 0.005*\"help\" + 0.004*\"give\"\n","Topic 9: 0.064*\"violenc\" + 0.044*\"intern\" + 0.034*\"conflict\" + 0.030*\"china\" + 0.020*\"russia\" + 0.019*\"peac\" + 0.017*\"cooper\" + 0.016*\"bank\" + 0.016*\"agreement\" + 0.016*\"border\" + 0.015*\"israel\" + 0.015*\"state\" + 0.015*\"india\" + 0.014*\"domest\" + 0.013*\"pizza\" + 0.011*\"neutral\" + 0.011*\"pineappl\" + 0.010*\"central\" + 0.010*\"region\" + 0.010*\"team\" + 0.009*\"west\" + 0.009*\"foreign\" + 0.009*\"sanction\" + 0.008*\"profess\" + 0.008*\"secur\" + 0.007*\"territori\" + 0.007*\"taxpay\" + 0.007*\"jew\" + 0.006*\"like\" + 0.006*\"recognit\" + 0.006*\"world\" + 0.006*\"monopoli\" + 0.006*\"palestinian\" + 0.006*\"iran\" + 0.006*\"nation\" + 0.006*\"jewish\" + 0.006*\"threat\" + 0.006*\"support\" + 0.006*\"dramat\" + 0.005*\"east\"\n","Topic 10: 0.054*\"game\" + 0.037*\"water\" + 0.026*\"distribut\" + 0.025*\"onlin\" + 0.023*\"imag\" + 0.022*\"video\" + 0.020*\"alien\" + 0.017*\"numer\" + 0.016*\"redistribut\" + 0.013*\"date\" + 0.013*\"thought\" + 0.012*\"celebr\" + 0.012*\"attach\" + 0.012*\"anonym\" + 0.011*\"drink\" + 0.011*\"creat\" + 0.010*\"revolut\" + 0.009*\"like\" + 0.009*\"creator\" + 0.009*\"blood\" + 0.009*\"lock\" + 0.009*\"comprehens\" + 0.008*\"real\" + 0.008*\"distinguish\" + 0.007*\"paint\" + 0.007*\"nazi\" + 0.007*\"luxuri\" + 0.007*\"protein\" + 0.006*\"plastic\" + 0.006*\"unreli\" + 0.006*\"luke\" + 0.006*\"form\" + 0.006*\"mankind\" + 0.006*\"milk\" + 0.006*\"guidelin\" + 0.006*\"detail\" + 0.006*\"season\" + 0.006*\"deleg\" + 0.005*\"fan\" + 0.005*\"contest\"\n","Topic 11: 0.067*\"stop\" + 0.054*\"murder\" + 0.038*\"diseas\" + 0.031*\"circumst\" + 0.025*\"die\" + 0.025*\"assist\" + 0.020*\"healthi\" + 0.019*\"extern\" + 0.019*\"japanes\" + 0.016*\"peopl\" + 0.015*\"minist\" + 0.015*\"permit\" + 0.014*\"priest\" + 0.014*\"likelihood\" + 0.013*\"pet\" + 0.013*\"lifestyl\" + 0.012*\"prime\" + 0.012*\"render\" + 0.012*\"forbid\" + 0.012*\"heart\" + 0.011*\"infant\" + 0.010*\"excus\" + 0.009*\"disciplin\" + 0.009*\"prevent\" + 0.008*\"infect\" + 0.008*\"factori\" + 0.008*\"circumcis\" + 0.008*\"routin\" + 0.007*\"healthier\" + 0.007*\"person\" + 0.007*\"weigh\" + 0.007*\"inappropri\" + 0.006*\"consid\" + 0.006*\"window\" + 0.006*\"live\" + 0.006*\"mortal\" + 0.006*\"youtub\" + 0.005*\"disregard\" + 0.005*\"bar\" + 0.005*\"hypocrit\"\n","Topic 12: 0.063*\"claim\" + 0.041*\"evid\" + 0.040*\"argument\" + 0.039*\"true\" + 0.026*\"subject\" + 0.025*\"object\" + 0.024*\"prove\" + 0.019*\"logic\" + 0.018*\"fact\" + 0.016*\"definit\" + 0.016*\"realiti\" + 0.015*\"reason\" + 0.015*\"standard\" + 0.015*\"assum\" + 0.014*\"fals\" + 0.013*\"truth\" + 0.013*\"exist\" + 0.011*\"statement\" + 0.011*\"valid\" + 0.011*\"support\" + 0.010*\"base\" + 0.010*\"assumpt\" + 0.009*\"mean\" + 0.009*\"proof\" + 0.008*\"point\" + 0.008*\"conclus\" + 0.008*\"ration\" + 0.007*\"make\" + 0.007*\"question\" + 0.006*\"fallaci\" + 0.006*\"case\" + 0.006*\"accept\" + 0.005*\"say\" + 0.005*\"irrelev\" + 0.005*\"assert\" + 0.005*\"virtual\" + 0.005*\"determin\" + 0.005*\"premis\" + 0.004*\"lack\" + 0.004*\"person\"\n","Topic 13: 0.067*\"crime\" + 0.058*\"legal\" + 0.041*\"commit\" + 0.041*\"illeg\" + 0.040*\"crimin\" + 0.038*\"punish\" + 0.022*\"type\" + 0.021*\"prison\" + 0.021*\"justic\" + 0.014*\"exclus\" + 0.012*\"convict\" + 0.011*\"assault\" + 0.011*\"sentenc\" + 0.010*\"case\" + 0.009*\"victim\" + 0.009*\"homicid\" + 0.009*\"prosecut\" + 0.008*\"offend\" + 0.008*\"mutual\" + 0.008*\"digit\" + 0.008*\"deterr\" + 0.008*\"societi\" + 0.008*\"death\" + 0.007*\"violent\" + 0.006*\"voluntari\" + 0.006*\"like\" + 0.006*\"rehabilit\" + 0.006*\"sexual\" + 0.006*\"transact\" + 0.006*\"murder\" + 0.006*\"deter\" + 0.006*\"report\" + 0.006*\"mean\" + 0.006*\"make\" + 0.005*\"prevent\" + 0.005*\"rape\" + 0.005*\"act\" + 0.005*\"person\" + 0.005*\"monitor\" + 0.005*\"arrest\"\n","Topic 14: 0.061*\"histori\" + 0.037*\"today\" + 0.036*\"histor\" + 0.036*\"modern\" + 0.026*\"past\" + 0.020*\"christian\" + 0.019*\"repar\" + 0.019*\"record\" + 0.018*\"charact\" + 0.017*\"write\" + 0.017*\"centuri\" + 0.015*\"time\" + 0.013*\"slaveri\" + 0.011*\"track\" + 0.011*\"attribut\" + 0.011*\"fashion\" + 0.011*\"roman\" + 0.010*\"memori\" + 0.010*\"confirm\" + 0.009*\"earli\" + 0.009*\"narrat\" + 0.008*\"invent\" + 0.008*\"empir\" + 0.008*\"mark\" + 0.008*\"overwhelm\" + 0.008*\"fire\" + 0.007*\"ancient\" + 0.007*\"greek\" + 0.007*\"present\" + 0.006*\"techniqu\" + 0.006*\"descend\" + 0.006*\"gospel\" + 0.006*\"civil\" + 0.006*\"genuin\" + 0.006*\"rememb\" + 0.006*\"confederaci\" + 0.006*\"wasn\" + 0.006*\"repeat\" + 0.005*\"exampl\" + 0.005*\"event\"\n","Topic 15: 0.084*\"year\" + 0.040*\"space\" + 0.036*\"earth\" + 0.032*\"poverti\" + 0.025*\"averag\" + 0.023*\"near\" + 0.023*\"time\" + 0.017*\"line\" + 0.016*\"planet\" + 0.016*\"wage\" + 0.015*\"size\" + 0.014*\"minimum\" + 0.013*\"move\" + 0.010*\"live\" + 0.010*\"thousand\" + 0.010*\"charg\" + 0.010*\"locat\" + 0.009*\"day\" + 0.009*\"life\" + 0.008*\"wait\" + 0.007*\"faster\" + 0.007*\"myth\" + 0.007*\"global\" + 0.006*\"calcul\" + 0.006*\"month\" + 0.006*\"apart\" + 0.006*\"number\" + 0.006*\"cycl\" + 0.006*\"hack\" + 0.006*\"doubl\" + 0.006*\"away\" + 0.005*\"like\" + 0.005*\"throw\" + 0.005*\"disappear\" + 0.005*\"station\" + 0.005*\"distanc\" + 0.005*\"take\" + 0.005*\"rise\" + 0.005*\"warm\" + 0.005*\"atmospher\"\n","Topic 16: 0.086*\"vote\" + 0.047*\"inform\" + 0.036*\"voter\" + 0.034*\"elect\" + 0.027*\"referendum\" + 0.022*\"peopl\" + 0.020*\"candid\" + 0.019*\"popular\" + 0.017*\"politician\" + 0.016*\"outcom\" + 0.014*\"prefer\" + 0.014*\"elector\" + 0.014*\"decid\" + 0.011*\"like\" + 0.011*\"alcohol\" + 0.010*\"decis\" + 0.010*\"public\" + 0.009*\"result\" + 0.009*\"campaign\" + 0.007*\"issu\" + 0.007*\"appeal\" + 0.006*\"count\" + 0.006*\"creativ\" + 0.006*\"make\" + 0.006*\"citizen\" + 0.006*\"smoke\" + 0.005*\"final\" + 0.005*\"lgbtq\" + 0.005*\"allow\" + 0.005*\"brexit\" + 0.005*\"polici\" + 0.005*\"base\" + 0.005*\"presidenti\" + 0.005*\"voic\" + 0.005*\"democraci\" + 0.005*\"give\" + 0.005*\"turnout\" + 0.005*\"time\" + 0.005*\"general\" + 0.005*\"seat\"\n","Topic 17: 0.092*\"state\" + 0.063*\"polit\" + 0.045*\"major\" + 0.036*\"parti\" + 0.026*\"democrat\" + 0.024*\"democraci\" + 0.023*\"unit\" + 0.023*\"govern\" + 0.022*\"repres\" + 0.020*\"member\" + 0.015*\"interest\" + 0.014*\"minor\" + 0.013*\"support\" + 0.011*\"nation\" + 0.010*\"polici\" + 0.010*\"legisl\" + 0.010*\"elect\" + 0.010*\"offici\" + 0.009*\"union\" + 0.008*\"power\" + 0.008*\"leader\" + 0.007*\"vast\" + 0.007*\"presid\" + 0.006*\"group\" + 0.006*\"failur\" + 0.006*\"influenc\" + 0.006*\"european\" + 0.006*\"oppos\" + 0.006*\"liber\" + 0.006*\"citizen\" + 0.005*\"congress\" + 0.005*\"direct\" + 0.005*\"parliament\" + 0.005*\"issu\" + 0.005*\"current\" + 0.004*\"public\" + 0.004*\"view\" + 0.004*\"represent\" + 0.004*\"senat\" + 0.004*\"form\"\n","Topic 18: 0.063*\"educ\" + 0.051*\"school\" + 0.045*\"student\" + 0.036*\"public\" + 0.035*\"fund\" + 0.032*\"program\" + 0.026*\"resourc\" + 0.025*\"learn\" + 0.021*\"teacher\" + 0.020*\"skill\" + 0.020*\"implement\" + 0.019*\"colleg\" + 0.017*\"privat\" + 0.017*\"institut\" + 0.014*\"financi\" + 0.013*\"global\" + 0.012*\"teach\" + 0.011*\"govern\" + 0.011*\"goal\" + 0.010*\"achiev\" + 0.009*\"better\" + 0.009*\"like\" + 0.008*\"help\" + 0.008*\"requir\" + 0.007*\"improv\" + 0.007*\"success\" + 0.006*\"academ\" + 0.006*\"support\" + 0.006*\"financ\" + 0.006*\"high\" + 0.005*\"allow\" + 0.005*\"sector\" + 0.005*\"import\" + 0.005*\"level\" + 0.005*\"commerci\" + 0.005*\"benefit\" + 0.004*\"merit\" + 0.004*\"effect\" + 0.004*\"develop\" + 0.004*\"alloc\"\n","Topic 19: 0.157*\"social\" + 0.046*\"behavior\" + 0.034*\"peopl\" + 0.030*\"construct\" + 0.026*\"worth\" + 0.021*\"enjoy\" + 0.020*\"capac\" + 0.019*\"talk\" + 0.018*\"perspect\" + 0.014*\"societi\" + 0.013*\"turkey\" + 0.012*\"display\" + 0.011*\"cognit\" + 0.010*\"white\" + 0.010*\"like\" + 0.009*\"color\" + 0.008*\"foster\" + 0.008*\"offens\" + 0.008*\"diminish\" + 0.007*\"consid\" + 0.007*\"surrend\" + 0.006*\"pope\" + 0.006*\"listen\" + 0.006*\"wider\" + 0.006*\"supremacist\" + 0.006*\"graduat\" + 0.006*\"misus\" + 0.006*\"discours\" + 0.006*\"genocid\" + 0.006*\"norm\" + 0.006*\"sound\" + 0.006*\"accept\" + 0.006*\"viewpoint\" + 0.005*\"sake\" + 0.005*\"creat\" + 0.005*\"slight\" + 0.005*\"cannib\" + 0.005*\"backlash\" + 0.005*\"skin\" + 0.005*\"narrow\"\n","Topic 20: 0.034*\"church\" + 0.029*\"author\" + 0.026*\"book\" + 0.024*\"jesus\" + 0.022*\"word\" + 0.022*\"bibl\" + 0.019*\"refer\" + 0.017*\"stori\" + 0.015*\"say\" + 0.013*\"interpret\" + 0.012*\"text\" + 0.011*\"read\" + 0.011*\"tell\" + 0.011*\"christian\" + 0.010*\"articl\" + 0.010*\"sourc\" + 0.009*\"write\" + 0.008*\"command\" + 0.008*\"liter\" + 0.008*\"mention\" + 0.008*\"origin\" + 0.008*\"document\" + 0.007*\"translat\" + 0.007*\"contain\" + 0.007*\"reliabl\" + 0.007*\"divin\" + 0.007*\"describ\" + 0.007*\"publish\" + 0.006*\"lie\" + 0.006*\"testament\" + 0.006*\"claim\" + 0.005*\"accord\" + 0.005*\"christ\" + 0.005*\"messiah\" + 0.005*\"mormon\" + 0.005*\"lord\" + 0.005*\"teach\" + 0.005*\"paper\" + 0.005*\"smith\" + 0.005*\"credibl\"\n","Topic 21: 0.058*\"deal\" + 0.023*\"larger\" + 0.020*\"ask\" + 0.019*\"british\" + 0.019*\"brexit\" + 0.018*\"ideal\" + 0.018*\"negoti\" + 0.016*\"struggl\" + 0.015*\"intervent\" + 0.014*\"mobil\" + 0.014*\"enhanc\" + 0.014*\"kid\" + 0.014*\"trend\" + 0.013*\"scenario\" + 0.013*\"compromis\" + 0.013*\"prepar\" + 0.012*\"version\" + 0.012*\"kant\" + 0.012*\"handl\" + 0.012*\"categor\" + 0.011*\"imper\" + 0.011*\"perpetu\" + 0.011*\"overpopul\" + 0.011*\"like\" + 0.010*\"divorc\" + 0.010*\"hope\" + 0.010*\"illustr\" + 0.009*\"termin\" + 0.009*\"allow\" + 0.009*\"guid\" + 0.008*\"mexico\" + 0.008*\"disast\" + 0.008*\"eichmann\" + 0.007*\"general\" + 0.007*\"deep\" + 0.007*\"feder\" + 0.007*\"deem\" + 0.007*\"distort\" + 0.007*\"react\" + 0.006*\"reason\"\n","Topic 22: 0.055*\"money\" + 0.050*\"cost\" + 0.044*\"spend\" + 0.034*\"million\" + 0.027*\"build\" + 0.025*\"save\" + 0.024*\"secur\" + 0.022*\"time\" + 0.022*\"militari\" + 0.019*\"plan\" + 0.017*\"oper\" + 0.017*\"billion\" + 0.017*\"effort\" + 0.016*\"total\" + 0.015*\"akm\" + 0.014*\"project\" + 0.013*\"estim\" + 0.010*\"amount\" + 0.010*\"year\" + 0.010*\"drone\" + 0.010*\"half\" + 0.010*\"wast\" + 0.010*\"govern\" + 0.010*\"zoo\" + 0.009*\"peopl\" + 0.009*\"budget\" + 0.008*\"requir\" + 0.008*\"resourc\" + 0.007*\"number\" + 0.006*\"cash\" + 0.006*\"expens\" + 0.006*\"bigger\" + 0.006*\"larg\" + 0.006*\"develop\" + 0.006*\"staff\" + 0.006*\"entertain\" + 0.005*\"small\" + 0.005*\"approxim\" + 0.005*\"world\" + 0.005*\"search\"\n","Topic 23: 0.037*\"generat\" + 0.035*\"design\" + 0.034*\"intellig\" + 0.028*\"complex\" + 0.022*\"select\" + 0.022*\"connect\" + 0.018*\"burden\" + 0.017*\"evolut\" + 0.016*\"simpl\" + 0.014*\"correl\" + 0.014*\"explain\" + 0.013*\"network\" + 0.012*\"expens\" + 0.012*\"artifici\" + 0.012*\"transport\" + 0.012*\"task\" + 0.012*\"evolv\" + 0.011*\"aris\" + 0.011*\"requir\" + 0.011*\"vehicl\" + 0.011*\"friend\" + 0.011*\"natur\" + 0.010*\"system\" + 0.009*\"unclear\" + 0.009*\"complic\" + 0.009*\"confeder\" + 0.008*\"dollar\" + 0.008*\"creat\" + 0.007*\"electr\" + 0.007*\"liquid\" + 0.007*\"evolutionari\" + 0.007*\"level\" + 0.006*\"develop\" + 0.006*\"rapid\" + 0.006*\"arriv\" + 0.005*\"last\" + 0.005*\"mean\" + 0.005*\"current\" + 0.005*\"causat\" + 0.005*\"rap\"\n","Topic 24: 0.053*\"problem\" + 0.047*\"increas\" + 0.039*\"product\" + 0.027*\"higher\" + 0.023*\"food\" + 0.019*\"rate\" + 0.018*\"lower\" + 0.018*\"produc\" + 0.018*\"cost\" + 0.017*\"meat\" + 0.016*\"demand\" + 0.016*\"rat\" + 0.015*\"price\" + 0.015*\"economi\" + 0.014*\"reduc\" + 0.014*\"consum\" + 0.013*\"trade\" + 0.012*\"level\" + 0.011*\"grow\" + 0.011*\"solv\" + 0.010*\"high\" + 0.010*\"growth\" + 0.010*\"consumpt\" + 0.010*\"popul\" + 0.009*\"lead\" + 0.008*\"farm\" + 0.008*\"unemploy\" + 0.008*\"environment\" + 0.008*\"sustain\" + 0.008*\"develop\" + 0.008*\"suppli\" + 0.007*\"solut\" + 0.007*\"diet\" + 0.007*\"effect\" + 0.007*\"world\" + 0.007*\"overal\" + 0.006*\"vegan\" + 0.006*\"peopl\" + 0.006*\"global\" + 0.006*\"softwar\"\n","Topic 25: 0.076*\"health\" + 0.061*\"mental\" + 0.031*\"cathol\" + 0.027*\"trust\" + 0.024*\"sport\" + 0.022*\"ill\" + 0.021*\"campaign\" + 0.019*\"exclud\" + 0.019*\"issu\" + 0.017*\"church\" + 0.016*\"dog\" + 0.014*\"reform\" + 0.014*\"disord\" + 0.013*\"center\" + 0.011*\"label\" + 0.011*\"injuri\" + 0.011*\"pick\" + 0.010*\"contact\" + 0.010*\"regist\" + 0.009*\"comment\" + 0.009*\"accid\" + 0.009*\"physic\" + 0.009*\"imprison\" + 0.009*\"help\" + 0.008*\"clinton\" + 0.008*\"evangel\" + 0.008*\"facil\" + 0.007*\"stage\" + 0.007*\"exampl\" + 0.007*\"inaccur\" + 0.007*\"depart\" + 0.007*\"condit\" + 0.007*\"peopl\" + 0.007*\"famous\" + 0.007*\"mix\" + 0.007*\"trillion\" + 0.006*\"honest\" + 0.006*\"public\" + 0.006*\"safeti\" + 0.006*\"drastic\"\n","Topic 26: 0.145*\"power\" + 0.114*\"experi\" + 0.046*\"love\" + 0.045*\"bring\" + 0.039*\"one\" + 0.019*\"peopl\" + 0.019*\"uniqu\" + 0.014*\"person\" + 0.014*\"unnecessari\" + 0.013*\"artist\" + 0.013*\"watch\" + 0.009*\"judgement\" + 0.009*\"like\" + 0.009*\"anymor\" + 0.008*\"mayb\" + 0.007*\"dictat\" + 0.007*\"porn\" + 0.006*\"zone\" + 0.006*\"make\" + 0.006*\"posit\" + 0.005*\"eas\" + 0.005*\"individu\" + 0.005*\"hasn\" + 0.005*\"investor\" + 0.004*\"especi\" + 0.004*\"hand\" + 0.004*\"give\" + 0.004*\"sudden\" + 0.004*\"strain\" + 0.004*\"insofar\" + 0.004*\"dream\" + 0.004*\"sens\" + 0.004*\"campus\" + 0.004*\"copyright\" + 0.004*\"knowledg\" + 0.004*\"familiar\" + 0.004*\"incomplet\" + 0.004*\"electron\" + 0.004*\"think\" + 0.004*\"similar\"\n","Topic 27: 0.237*\"human\" + 0.095*\"anim\" + 0.058*\"life\" + 0.049*\"kill\" + 0.047*\"natur\" + 0.037*\"live\" + 0.023*\"speci\" + 0.014*\"be\" + 0.009*\"like\" + 0.009*\"eat\" + 0.008*\"surviv\" + 0.007*\"breed\" + 0.007*\"extinct\" + 0.007*\"differ\" + 0.006*\"environ\" + 0.006*\"food\" + 0.006*\"plant\" + 0.006*\"consid\" + 0.006*\"wild\" + 0.006*\"meat\" + 0.005*\"save\" + 0.005*\"capabl\" + 0.005*\"bitcoin\" + 0.005*\"suffer\" + 0.005*\"organ\" + 0.005*\"import\" + 0.004*\"similar\" + 0.004*\"creat\" + 0.004*\"condit\" + 0.004*\"predat\" + 0.003*\"dedic\" + 0.003*\"ecosystem\" + 0.003*\"purpos\" + 0.003*\"tast\" + 0.003*\"shelter\" + 0.003*\"abil\" + 0.003*\"potenti\" + 0.003*\"reason\" + 0.003*\"whale\" + 0.003*\"make\"\n","Topic 28: 0.079*\"activ\" + 0.042*\"particip\" + 0.036*\"engag\" + 0.032*\"peopl\" + 0.030*\"partner\" + 0.029*\"citi\" + 0.029*\"multipl\" + 0.020*\"applic\" + 0.018*\"regular\" + 0.018*\"vulner\" + 0.017*\"sexual\" + 0.015*\"travel\" + 0.014*\"girl\" + 0.014*\"automat\" + 0.012*\"attend\" + 0.011*\"wikipedia\" + 0.011*\"allow\" + 0.010*\"time\" + 0.010*\"innoc\" + 0.010*\"explor\" + 0.010*\"york\" + 0.009*\"like\" + 0.009*\"visit\" + 0.009*\"extra\" + 0.008*\"transpar\" + 0.008*\"interven\" + 0.008*\"mar\" + 0.007*\"safe\" + 0.007*\"encourag\" + 0.006*\"boy\" + 0.006*\"facilit\" + 0.006*\"permiss\" + 0.006*\"asexu\" + 0.006*\"fault\" + 0.006*\"replic\" + 0.006*\"embrac\" + 0.006*\"have\" + 0.006*\"minut\" + 0.005*\"alleg\" + 0.005*\"secondari\"\n","Topic 29: 0.143*\"right\" + 0.048*\"protect\" + 0.032*\"freedom\" + 0.028*\"citizen\" + 0.024*\"constitut\" + 0.022*\"individu\" + 0.020*\"properti\" + 0.019*\"govern\" + 0.019*\"state\" + 0.018*\"court\" + 0.017*\"violat\" + 0.016*\"restrict\" + 0.013*\"peopl\" + 0.013*\"civil\" + 0.013*\"legal\" + 0.012*\"amend\" + 0.012*\"person\" + 0.011*\"societi\" + 0.010*\"grant\" + 0.010*\"duti\" + 0.009*\"principl\" + 0.009*\"suprem\" + 0.008*\"guarante\" + 0.008*\"fundament\" + 0.008*\"contract\" + 0.007*\"limit\" + 0.007*\"exercis\" + 0.007*\"mean\" + 0.007*\"declar\" + 0.007*\"liberti\" + 0.006*\"autonomi\" + 0.006*\"allow\" + 0.006*\"case\" + 0.005*\"interfer\" + 0.005*\"life\" + 0.005*\"basic\" + 0.005*\"decid\" + 0.005*\"reason\" + 0.005*\"give\" + 0.004*\"absolut\"\n","Topic 30: 0.167*\"countri\" + 0.059*\"popul\" + 0.055*\"american\" + 0.025*\"immigr\" + 0.020*\"world\" + 0.019*\"refuge\" + 0.019*\"germani\" + 0.016*\"administr\" + 0.014*\"home\" + 0.013*\"percent\" + 0.012*\"develop\" + 0.011*\"declin\" + 0.011*\"african\" + 0.009*\"hide\" + 0.009*\"citizen\" + 0.009*\"crisi\" + 0.009*\"live\" + 0.008*\"german\" + 0.008*\"high\" + 0.008*\"number\" + 0.007*\"learn\" + 0.007*\"nativ\" + 0.007*\"resid\" + 0.006*\"like\" + 0.006*\"terribl\" + 0.006*\"muslim\" + 0.006*\"exampl\" + 0.005*\"africa\" + 0.005*\"take\" + 0.005*\"worldwid\" + 0.005*\"larg\" + 0.005*\"host\" + 0.005*\"art\" + 0.005*\"migrat\" + 0.005*\"franc\" + 0.005*\"compar\" + 0.004*\"signific\" + 0.004*\"ecolog\" + 0.004*\"current\" + 0.004*\"iraq\"\n","Topic 31: 0.045*\"fight\" + 0.045*\"attack\" + 0.040*\"carri\" + 0.036*\"train\" + 0.031*\"defens\" + 0.030*\"tool\" + 0.029*\"arm\" + 0.026*\"forc\" + 0.019*\"soldier\" + 0.018*\"actor\" + 0.018*\"shouldn\" + 0.017*\"terrorist\" + 0.017*\"civilian\" + 0.017*\"armi\" + 0.014*\"combat\" + 0.014*\"terror\" + 0.014*\"militari\" + 0.014*\"defend\" + 0.013*\"hire\" + 0.011*\"threat\" + 0.010*\"bull\" + 0.008*\"suppress\" + 0.008*\"effect\" + 0.008*\"felt\" + 0.008*\"battl\" + 0.007*\"like\" + 0.007*\"habit\" + 0.007*\"partial\" + 0.006*\"johnson\" + 0.006*\"militia\" + 0.006*\"lifetim\" + 0.006*\"adjust\" + 0.006*\"mine\" + 0.006*\"tackl\" + 0.005*\"unrealist\" + 0.005*\"recruit\" + 0.005*\"weak\" + 0.005*\"situat\" + 0.005*\"warfar\" + 0.005*\"casualti\"\n","Topic 32: 0.069*\"trump\" + 0.060*\"decis\" + 0.031*\"report\" + 0.030*\"data\" + 0.030*\"media\" + 0.026*\"presid\" + 0.021*\"user\" + 0.021*\"collect\" + 0.020*\"make\" + 0.019*\"republican\" + 0.018*\"content\" + 0.017*\"bias\" + 0.016*\"platform\" + 0.012*\"conserv\" + 0.012*\"poll\" + 0.010*\"public\" + 0.009*\"support\" + 0.008*\"inform\" + 0.008*\"approv\" + 0.008*\"base\" + 0.008*\"obama\" + 0.008*\"sort\" + 0.007*\"strategi\" + 0.007*\"evalu\" + 0.007*\"comput\" + 0.006*\"withdraw\" + 0.006*\"sourc\" + 0.006*\"reput\" + 0.006*\"googl\" + 0.006*\"websit\" + 0.006*\"polici\" + 0.006*\"detect\" + 0.005*\"influenc\" + 0.005*\"view\" + 0.005*\"brand\" + 0.005*\"lot\" + 0.004*\"tactic\" + 0.004*\"like\" + 0.004*\"bisexu\" + 0.004*\"accord\"\n","Topic 33: 0.130*\"religion\" + 0.078*\"religi\" + 0.060*\"valu\" + 0.056*\"believ\" + 0.048*\"belief\" + 0.035*\"peopl\" + 0.024*\"faith\" + 0.017*\"teach\" + 0.016*\"christian\" + 0.013*\"secular\" + 0.010*\"practic\" + 0.010*\"view\" + 0.010*\"differ\" + 0.009*\"follow\" + 0.009*\"base\" + 0.009*\"societi\" + 0.009*\"reason\" + 0.008*\"think\" + 0.008*\"islam\" + 0.007*\"ideolog\" + 0.007*\"exampl\" + 0.007*\"promot\" + 0.007*\"world\" + 0.006*\"adher\" + 0.006*\"hold\" + 0.006*\"life\" + 0.005*\"core\" + 0.005*\"spiritu\" + 0.005*\"person\" + 0.005*\"atheist\" + 0.005*\"mean\" + 0.005*\"philosophi\" + 0.004*\"individu\" + 0.004*\"tenet\" + 0.004*\"cruel\" + 0.003*\"atheism\" + 0.003*\"organ\" + 0.003*\"ethic\" + 0.003*\"live\" + 0.003*\"atroc\"\n","Topic 34: 0.278*\"need\" + 0.029*\"peopl\" + 0.028*\"corrupt\" + 0.027*\"scale\" + 0.016*\"emerg\" + 0.012*\"blame\" + 0.012*\"scheme\" + 0.011*\"viabl\" + 0.011*\"satisfi\" + 0.011*\"fulfil\" + 0.011*\"order\" + 0.010*\"press\" + 0.010*\"larg\" + 0.010*\"older\" + 0.009*\"like\" + 0.009*\"piec\" + 0.009*\"hundr\" + 0.008*\"gather\" + 0.007*\"touch\" + 0.007*\"usag\" + 0.007*\"demograph\" + 0.007*\"younger\" + 0.007*\"realist\" + 0.007*\"forgiv\" + 0.006*\"section\" + 0.006*\"optim\" + 0.006*\"hungari\" + 0.006*\"vegan\" + 0.006*\"desper\" + 0.005*\"hierarchi\" + 0.005*\"want\" + 0.005*\"medium\" + 0.005*\"toxic\" + 0.005*\"live\" + 0.005*\"humankind\" + 0.005*\"simul\" + 0.005*\"millenni\" + 0.005*\"forum\" + 0.004*\"anecdot\" + 0.004*\"virus\"\n","Topic 35: 0.062*\"gun\" + 0.044*\"ban\" + 0.035*\"tortur\" + 0.031*\"firearm\" + 0.031*\"materi\" + 0.026*\"bullfight\" + 0.024*\"stress\" + 0.024*\"labour\" + 0.021*\"attract\" + 0.021*\"wear\" + 0.020*\"hunt\" + 0.020*\"sever\" + 0.017*\"bomb\" + 0.015*\"sacrific\" + 0.015*\"cloth\" + 0.012*\"abandon\" + 0.012*\"excess\" + 0.012*\"primarili\" + 0.011*\"acquir\" + 0.010*\"discourag\" + 0.010*\"spain\" + 0.009*\"censorship\" + 0.008*\"pictur\" + 0.008*\"endors\" + 0.008*\"late\" + 0.008*\"uniform\" + 0.008*\"distress\" + 0.007*\"spanish\" + 0.007*\"peopl\" + 0.007*\"legaci\" + 0.007*\"cut\" + 0.007*\"dress\" + 0.006*\"resort\" + 0.006*\"site\" + 0.006*\"injur\" + 0.005*\"print\" + 0.005*\"canon\" + 0.005*\"invad\" + 0.005*\"practic\" + 0.005*\"rock\"\n","Topic 36: 0.144*\"chang\" + 0.034*\"discuss\" + 0.028*\"climat\" + 0.020*\"normal\" + 0.019*\"time\" + 0.019*\"debat\" + 0.016*\"topic\" + 0.016*\"opinion\" + 0.015*\"traffic\" + 0.013*\"adapt\" + 0.013*\"norm\" + 0.012*\"expert\" + 0.012*\"issu\" + 0.012*\"understand\" + 0.011*\"societ\" + 0.010*\"peopl\" + 0.010*\"societi\" + 0.009*\"moment\" + 0.009*\"orient\" + 0.008*\"stigma\" + 0.008*\"like\" + 0.008*\"alter\" + 0.008*\"sensit\" + 0.008*\"pattern\" + 0.007*\"percept\" + 0.007*\"public\" + 0.007*\"current\" + 0.007*\"beauti\" + 0.006*\"happen\" + 0.006*\"preval\" + 0.006*\"suit\" + 0.006*\"differ\" + 0.006*\"controversi\" + 0.006*\"affect\" + 0.006*\"view\" + 0.006*\"import\" + 0.005*\"behaviour\" + 0.005*\"insuffici\" + 0.005*\"make\" + 0.005*\"chain\"\n","Topic 37: 0.109*\"children\" + 0.075*\"parent\" + 0.062*\"child\" + 0.053*\"famili\" + 0.043*\"abort\" + 0.023*\"bear\" + 0.021*\"adopt\" + 0.019*\"life\" + 0.018*\"birth\" + 0.018*\"mother\" + 0.017*\"adult\" + 0.013*\"rais\" + 0.013*\"disabl\" + 0.012*\"young\" + 0.011*\"babi\" + 0.010*\"have\" + 0.010*\"airbnb\" + 0.009*\"care\" + 0.009*\"live\" + 0.009*\"father\" + 0.007*\"want\" + 0.007*\"like\" + 0.007*\"abl\" + 0.006*\"woman\" + 0.006*\"allow\" + 0.006*\"support\" + 0.006*\"case\" + 0.006*\"adequ\" + 0.005*\"give\" + 0.005*\"forc\" + 0.005*\"home\" + 0.005*\"consent\" + 0.005*\"reason\" + 0.005*\"prospect\" + 0.005*\"member\" + 0.005*\"time\" + 0.004*\"later\" + 0.004*\"grow\" + 0.004*\"respons\" + 0.004*\"decis\"\n","Topic 38: 0.071*\"self\" + 0.052*\"black\" + 0.049*\"white\" + 0.045*\"polic\" + 0.035*\"mass\" + 0.034*\"race\" + 0.032*\"shoot\" + 0.028*\"peopl\" + 0.025*\"offic\" + 0.023*\"interact\" + 0.022*\"american\" + 0.022*\"awar\" + 0.018*\"racial\" + 0.016*\"racism\" + 0.013*\"convers\" + 0.011*\"racist\" + 0.011*\"like\" + 0.010*\"aggress\" + 0.008*\"shooter\" + 0.008*\"confid\" + 0.007*\"individu\" + 0.007*\"therapi\" + 0.007*\"minor\" + 0.007*\"prejudic\" + 0.007*\"equat\" + 0.006*\"guilt\" + 0.006*\"cannabi\" + 0.006*\"forc\" + 0.005*\"activist\" + 0.005*\"group\" + 0.005*\"gang\" + 0.005*\"discrimin\" + 0.005*\"succeed\" + 0.005*\"profil\" + 0.005*\"door\" + 0.005*\"exhibit\" + 0.004*\"person\" + 0.004*\"brutal\" + 0.004*\"issu\" + 0.004*\"enslav\"\n","Topic 39: 0.047*\"econom\" + 0.042*\"incom\" + 0.025*\"poor\" + 0.024*\"peopl\" + 0.023*\"welfar\" + 0.022*\"busi\" + 0.021*\"tax\" + 0.020*\"wealth\" + 0.017*\"invest\" + 0.016*\"capit\" + 0.015*\"money\" + 0.015*\"incent\" + 0.014*\"benefit\" + 0.014*\"progress\" + 0.013*\"inequ\" + 0.013*\"societi\" + 0.012*\"economi\" + 0.012*\"labor\" + 0.012*\"earn\" + 0.011*\"increas\" + 0.011*\"rich\" + 0.011*\"benefici\" + 0.010*\"player\" + 0.010*\"govern\" + 0.009*\"exploit\" + 0.009*\"creat\" + 0.009*\"wealthi\" + 0.009*\"prostitut\" + 0.009*\"degre\" + 0.008*\"high\" + 0.008*\"contribut\" + 0.008*\"reduc\" + 0.008*\"revenu\" + 0.007*\"individu\" + 0.007*\"owner\" + 0.007*\"financi\" + 0.007*\"taxat\" + 0.007*\"worker\" + 0.007*\"make\" + 0.007*\"gain\"\n","Topic 40: 0.075*\"bodi\" + 0.047*\"function\" + 0.044*\"process\" + 0.039*\"brain\" + 0.028*\"part\" + 0.024*\"structur\" + 0.020*\"send\" + 0.017*\"check\" + 0.016*\"messag\" + 0.016*\"dead\" + 0.015*\"manner\" + 0.013*\"block\" + 0.012*\"chemic\" + 0.011*\"insid\" + 0.011*\"revers\" + 0.011*\"burn\" + 0.010*\"overcom\" + 0.009*\"background\" + 0.008*\"cell\" + 0.008*\"elder\" + 0.008*\"contemporari\" + 0.007*\"footbal\" + 0.007*\"night\" + 0.007*\"miss\" + 0.007*\"pregnant\" + 0.006*\"visibl\" + 0.006*\"uncomfort\" + 0.006*\"journal\" + 0.006*\"organ\" + 0.006*\"damag\" + 0.006*\"frame\" + 0.005*\"underground\" + 0.005*\"eye\" + 0.005*\"wave\" + 0.005*\"soft\" + 0.005*\"ahead\" + 0.005*\"convert\" + 0.005*\"nutrient\" + 0.005*\"restor\" + 0.004*\"batteri\"\n","Topic 41: 0.067*\"studi\" + 0.064*\"show\" + 0.057*\"risk\" + 0.046*\"research\" + 0.025*\"effect\" + 0.025*\"link\" + 0.020*\"increas\" + 0.018*\"decreas\" + 0.016*\"suggest\" + 0.015*\"vaccin\" + 0.013*\"statist\" + 0.012*\"pregnanc\" + 0.012*\"impact\" + 0.011*\"number\" + 0.011*\"signific\" + 0.009*\"negat\" + 0.008*\"reduc\" + 0.007*\"mitig\" + 0.007*\"cite\" + 0.007*\"sit\" + 0.007*\"factor\" + 0.007*\"contracept\" + 0.007*\"result\" + 0.007*\"age\" + 0.007*\"depress\" + 0.007*\"young\" + 0.007*\"widespread\" + 0.007*\"develop\" + 0.007*\"high\" + 0.006*\"exposur\" + 0.006*\"pose\" + 0.006*\"indic\" + 0.006*\"posit\" + 0.006*\"tendenc\" + 0.006*\"fetus\" + 0.006*\"accord\" + 0.006*\"percentag\" + 0.006*\"like\" + 0.006*\"associ\" + 0.005*\"recent\"\n","Topic 42: 0.119*\"free\" + 0.075*\"idea\" + 0.046*\"speech\" + 0.041*\"languag\" + 0.026*\"speak\" + 0.021*\"hate\" + 0.021*\"ignor\" + 0.021*\"communic\" + 0.020*\"express\" + 0.019*\"creation\" + 0.013*\"symbol\" + 0.012*\"japan\" + 0.011*\"necess\" + 0.011*\"constant\" + 0.010*\"peopl\" + 0.009*\"allow\" + 0.008*\"precis\" + 0.008*\"trial\" + 0.008*\"arbitrari\" + 0.007*\"limit\" + 0.007*\"speaker\" + 0.007*\"english\" + 0.007*\"censor\" + 0.007*\"word\" + 0.006*\"mean\" + 0.006*\"accomplish\" + 0.006*\"boundari\" + 0.006*\"clinic\" + 0.006*\"sentient\" + 0.006*\"atom\" + 0.005*\"behav\" + 0.005*\"somebodi\" + 0.005*\"oneself\" + 0.005*\"concept\" + 0.005*\"creat\" + 0.005*\"flow\" + 0.005*\"endur\" + 0.005*\"persist\" + 0.004*\"differ\" + 0.004*\"defin\"\n","Topic 43: 0.082*\"cultur\" + 0.064*\"group\" + 0.049*\"communiti\" + 0.043*\"nation\" + 0.020*\"share\" + 0.020*\"tradit\" + 0.018*\"differ\" + 0.017*\"divers\" + 0.017*\"western\" + 0.016*\"north\" + 0.016*\"european\" + 0.014*\"europ\" + 0.013*\"korea\" + 0.012*\"integr\" + 0.012*\"oppress\" + 0.011*\"privileg\" + 0.011*\"lgbt\" + 0.010*\"societi\" + 0.010*\"peopl\" + 0.010*\"america\" + 0.010*\"shift\" + 0.009*\"harass\" + 0.009*\"south\" + 0.009*\"preserv\" + 0.009*\"minor\" + 0.009*\"common\" + 0.009*\"member\" + 0.008*\"ident\" + 0.008*\"ethnic\" + 0.007*\"bathroom\" + 0.007*\"world\" + 0.007*\"local\" + 0.007*\"belong\" + 0.006*\"creat\" + 0.006*\"recognis\" + 0.006*\"coloni\" + 0.005*\"individu\" + 0.005*\"like\" + 0.005*\"form\" + 0.005*\"accept\"\n","Topic 44: 0.093*\"long\" + 0.089*\"term\" + 0.057*\"pay\" + 0.030*\"time\" + 0.025*\"short\" + 0.025*\"period\" + 0.019*\"compens\" + 0.019*\"payment\" + 0.016*\"monument\" + 0.015*\"salari\" + 0.014*\"attent\" + 0.012*\"statu\" + 0.012*\"mistak\" + 0.012*\"destruct\" + 0.011*\"room\" + 0.011*\"public\" + 0.011*\"peer\" + 0.010*\"compel\" + 0.010*\"nuditi\" + 0.010*\"worship\" + 0.010*\"inherit\" + 0.010*\"review\" + 0.009*\"asset\" + 0.008*\"honor\" + 0.008*\"remov\" + 0.007*\"royal\" + 0.007*\"conscript\" + 0.007*\"arrang\" + 0.006*\"place\" + 0.006*\"peopl\" + 0.005*\"ancestor\" + 0.005*\"voluntarili\" + 0.005*\"leagu\" + 0.005*\"screen\" + 0.005*\"retir\" + 0.005*\"master\" + 0.004*\"programm\" + 0.004*\"earlier\" + 0.004*\"fin\" + 0.004*\"receiv\"\n","Topic 45: 0.068*\"action\" + 0.065*\"law\" + 0.047*\"respons\" + 0.046*\"death\" + 0.041*\"control\" + 0.041*\"govern\" + 0.037*\"rule\" + 0.027*\"enforc\" + 0.026*\"regul\" + 0.023*\"marriag\" + 0.020*\"polici\" + 0.019*\"break\" + 0.012*\"peopl\" + 0.012*\"impos\" + 0.011*\"affirm\" + 0.011*\"prohibit\" + 0.010*\"societi\" + 0.009*\"exempt\" + 0.009*\"strict\" + 0.008*\"prevent\" + 0.008*\"hold\" + 0.007*\"allow\" + 0.007*\"penalti\" + 0.007*\"toler\" + 0.007*\"take\" + 0.006*\"place\" + 0.006*\"mandat\" + 0.006*\"account\" + 0.006*\"catch\" + 0.006*\"follow\" + 0.005*\"state\" + 0.005*\"standard\" + 0.005*\"individu\" + 0.005*\"effect\" + 0.005*\"enact\" + 0.005*\"forc\" + 0.005*\"creat\" + 0.005*\"abolish\" + 0.004*\"legisl\" + 0.004*\"order\"\n","Topic 46: 0.100*\"provid\" + 0.055*\"choic\" + 0.049*\"servic\" + 0.035*\"market\" + 0.032*\"care\" + 0.019*\"peopl\" + 0.018*\"fair\" + 0.018*\"benefit\" + 0.018*\"offer\" + 0.017*\"profit\" + 0.016*\"govern\" + 0.015*\"healthcar\" + 0.014*\"competit\" + 0.013*\"land\" + 0.013*\"qualiti\" + 0.012*\"compani\" + 0.011*\"health\" + 0.011*\"corpor\" + 0.010*\"own\" + 0.010*\"purchas\" + 0.010*\"individu\" + 0.009*\"custom\" + 0.009*\"free\" + 0.009*\"privat\" + 0.008*\"innov\" + 0.008*\"afford\" + 0.008*\"insur\" + 0.008*\"zero\" + 0.007*\"better\" + 0.007*\"mean\" + 0.007*\"allow\" + 0.006*\"busi\" + 0.006*\"provis\" + 0.006*\"exchang\" + 0.005*\"public\" + 0.005*\"ownership\" + 0.005*\"trade\" + 0.005*\"option\" + 0.005*\"forc\" + 0.005*\"give\"\n","Topic 47: 0.107*\"women\" + 0.061*\"gender\" + 0.051*\"equal\" + 0.034*\"sexual\" + 0.026*\"role\" + 0.022*\"femal\" + 0.021*\"male\" + 0.020*\"play\" + 0.019*\"ident\" + 0.018*\"biolog\" + 0.017*\"discrimin\" + 0.016*\"differ\" + 0.013*\"woman\" + 0.012*\"societi\" + 0.008*\"tran\" + 0.008*\"stereotyp\" + 0.008*\"peopl\" + 0.007*\"disadvantag\" + 0.007*\"athlet\" + 0.007*\"individu\" + 0.007*\"transgend\" + 0.007*\"career\" + 0.006*\"opportun\" + 0.006*\"domin\" + 0.006*\"base\" + 0.006*\"femin\" + 0.005*\"characterist\" + 0.005*\"workplac\" + 0.005*\"categori\" + 0.005*\"mean\" + 0.005*\"reproduct\" + 0.005*\"assign\" + 0.005*\"like\" + 0.004*\"reinforc\" + 0.004*\"feminist\" + 0.004*\"level\" + 0.004*\"strength\" + 0.004*\"physic\" + 0.004*\"give\" + 0.004*\"import\"\n","Topic 48: 0.110*\"good\" + 0.073*\"thing\" + 0.070*\"know\" + 0.055*\"evil\" + 0.022*\"peopl\" + 0.021*\"desir\" + 0.020*\"world\" + 0.016*\"mean\" + 0.016*\"possibl\" + 0.014*\"creat\" + 0.012*\"robot\" + 0.011*\"omnipot\" + 0.011*\"greater\" + 0.011*\"happi\" + 0.010*\"necessarili\" + 0.010*\"reason\" + 0.010*\"want\" + 0.009*\"marri\" + 0.008*\"logic\" + 0.008*\"omnisci\" + 0.008*\"best\" + 0.007*\"understand\" + 0.007*\"exist\" + 0.007*\"maxim\" + 0.006*\"person\" + 0.006*\"answer\" + 0.006*\"better\" + 0.006*\"think\" + 0.005*\"abl\" + 0.005*\"imagin\" + 0.005*\"exampl\" + 0.005*\"live\" + 0.005*\"allow\" + 0.005*\"give\" + 0.005*\"entail\" + 0.005*\"reaction\" + 0.005*\"hell\" + 0.005*\"futur\" + 0.005*\"heaven\" + 0.004*\"necessari\"\n","Topic 49: 0.132*\"work\" + 0.048*\"drug\" + 0.038*\"access\" + 0.032*\"worker\" + 0.030*\"compani\" + 0.026*\"peopl\" + 0.026*\"employ\" + 0.025*\"avail\" + 0.020*\"industri\" + 0.016*\"legal\" + 0.015*\"employe\" + 0.014*\"like\" + 0.013*\"job\" + 0.012*\"internet\" + 0.011*\"addict\" + 0.010*\"hour\" + 0.010*\"sell\" + 0.008*\"legalis\" + 0.008*\"allow\" + 0.008*\"market\" + 0.007*\"crop\" + 0.007*\"australia\" + 0.007*\"regul\" + 0.007*\"buy\" + 0.006*\"hard\" + 0.006*\"time\" + 0.006*\"advertis\" + 0.006*\"increas\" + 0.005*\"mean\" + 0.005*\"inflat\" + 0.005*\"make\" + 0.005*\"store\" + 0.005*\"help\" + 0.005*\"household\" + 0.005*\"abl\" + 0.004*\"reduc\" + 0.004*\"product\" + 0.004*\"medicin\" + 0.004*\"current\" + 0.004*\"unpaid\"\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\n# Get entities for each sentence and add for the topics\\ndictionary = gensim.corpora.Dictionary(all_docs)\\nfor idx in tqdm(range(len(all_docs)), ascii=True):\\n  doc = all_docs[idx]\\n  sent = all_sents[idx]\\n  corpus = [dictionary.doc2bow(doc)]\\n  top_topics = (\\n      lda_model.get_document_topics(corpus, minimum_probability=0.0)\\n  )\\n  # Pick top topic for adding entities\\n  top_topic = sorted(top_topics[0], key=lambda x: x[1], reverse=True)[0]\\n  top_topic_id = top_topic[0]\\n\\n  entities = []\\n  tk_sents = sent_tokenize(sent)\\n  for ss in tk_sents:\\n    e1, e2 = extract_entities(ss)\\n    entities.append((e1, e2))\\n  \\n  # Add to the current topic as a single entry\\n  topic_wise_entities[top_topic_id].append(entities)\\n\\ntopic_entities_json = os.path.join(drive_path, 'kialo_topics', 'topic_entities.json')\\nwith open(topic_entities_json, 'w') as f:\\n  json.dump(topic_wise_entities, f)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["\n","topics_path = os.path.join(drive_path, 'topics_overall')\n","\n","import spacy\n","nlp = spacy.load('en_core_web_lg')\n","\n","from spacy.tokens import Span\n","from spacy.matcher import Matcher\n","\n","nltk.download('punkt')\n","\n","topic_model_path = os.path.join(drive_path, 'kialo_topics', 'lda_kialo_topics.ckpt')\n","lda_model = gensim.models.LdaMulticore.load(topic_model_path)\n","\n","def extract_entities(sents):\n","   global nlp\n","   # chunk one\n","   enti_one = \"\"\n","   enti_two = \"\"\n","  \n","   dep_prev_token = \"\" # dependency tag of previous token in sentence\n","  \n","   txt_prev_token = \"\" # previous token in sentence\n","  \n","   prefix = \"\"\n","   modifier = \"\"\n","  \n","   for tokn in nlp(sents):\n","       # chunk two\n","       ## move to next token if token is punctuation\n","      \n","       if tokn.dep_ != \"punct\":\n","           #  check if token is compound word or not\n","           if tokn.dep_ == \"compound\":\n","               prefix = tokn.text\n","               # add the current word to it if the previous word is 'compound’\n","               if dep_prev_token == \"compound\":\n","                   prefix = txt_prev_token + \" \"+ tokn.text\n","                  \n","           # verify if token is modifier or not\n","           if tokn.dep_.endswith(\"mod\") == True:\n","               modifier = tokn.text\n","               # add it to the current word if the previous word is 'compound'\n","               if dep_prev_token == \"compound\":\n","                   modifier = txt_prev_token + \" \"+ tokn.text\n","                  \n","           # chunk3\n","           if tokn.dep_.find(\"subj\") == True:\n","               enti_one = modifier +\" \"+ prefix + \" \"+ tokn.text\n","               prefix = \"\"\n","               modifier = \"\"\n","               dep_prev_token = \"\"\n","               txt_prev_token = \"\"\n","              \n","           # chunk4\n","           if tokn.dep_.find(\"obj\") == True:\n","               enti_two = modifier +\" \"+ prefix +\" \"+ tokn.text\n","              \n","           # chunk 5\n","           # update variable\n","           dep_prev_token = tokn.dep_\n","           txt_prev_token = tokn.text\n","          \n","   return [enti_one.strip(), enti_two.strip()]\n","\n","\n","os.makedirs(topics_path, exist_ok=True)\n","\n","topic_wise_entities = {\n","    i: [] for i in range(num_topics)\n","}\n","topic_wise_words = {\n","    i: [] for i in range(num_topics)\n","}\n","\n","from nltk.tokenize import sent_tokenize\n","\n","# Get top-40 words for each topic\n","for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=40):\n","  topic_words_raw = [x.strip().split('*')[-1] for x in topic.strip().split('+')]\n","  topic_words_mapped = [word_map.get(x.replace('\"', '')) for x in topic_words_raw]\n","  topic_words = [min(x, key=len) if x else topic_words_raw[i] for i, x in enumerate(topic_words_mapped)]\n","  print(f\"Topic {idx}: {topic}\")\n","  topic_wise_words[idx] = topic_words\n","\n","# print(topic_wise_words)\n","topic_words_json = os.path.join(drive_path, 'kialo_topics', 'topic_words.json')\n","with open(topic_words_json, 'w') as f:\n","  json.dump(topic_wise_words, f)\n","\n"]},{"cell_type":"code","source":["import sys\n","import os\n","print(os.path.abspath('.'))\n","def generate_bow(input_sentence, aspect):\n","  # Use topic model to find input sentence's topic, get the words and entities matching\n","  # the ones in input sentence and use that BoW txt for inference.\n","\n","  dictionary = gensim.corpora.Dictionary(all_docs)\n","\n","  bow_topic = os.path.join(drive_path, 'PPLM', 'arg_gen', 'b1.txt')\n","  # bow_ent = os.path.join(drive_path, 'PPLM', 'arg_gen', 'bow_ent.txt')\n","\n","  tokens = sentence_to_seq(input_sentence)\n","  corpus = [dictionary.doc2bow(tokens)]\n","  top_topics = (\n","      lda_model.get_document_topics(corpus, minimum_probability=0.0)\n","  )\n","  # Pick top topic for adding entities\n","  top_topic = sorted(top_topics[0], key=lambda x: x[1], reverse=True)[0]\n","  top_topic_id = top_topic[0]\n","\n","  words_l = topic_wise_words[top_topic_id]\n","\n","from subprocess import Popen, PIPE\n","\n","def run_model(\n","    cond_text, grad_len=30, length=50, stepsize=0.01, kl_scale=0.09,\n","    num_samples=5, window_length=10, idx=1\n","):\n","\n","  with open(os.path.join(drive_path,'PPLM', f'arg_gen_outputs_{idx}.txt'), 'ab') as f:\n","    process = Popen([\n","      'python', 'run_pplm.py', '-B', './arg_gen/b1.txt', '-D', 'generic', '--window_length', f'{window_length}',\n","      '--class_label', '0', '--cond_text', f'{cond_text}', '--grad_length', f'{grad_len}',\n","      '--length', f'{length}', '--gamma', '1.0', '--num_iterations', '5', '--num_samples', f'{num_samples}',\n","      '--stepsize', f'{stepsize}', '--kl_scale', f'{kl_scale}', '--gm_scale', '0.99', '--colorama',\n","      '--sample', '--discrim_weights', '/content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_epoch_50.pt',\n","      '--discrim_meta', '/content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_meta.json',\n","      '--verbosity', 'quiet'\n","    ], stdout=PIPE)\n","    for line in iter(process.stdout.readline, b\"\"):\n","      sys.stdout.write(line)\n","      f.write(line)\n","\n","  #os.system(\n","  #    f\"python run_pplm.py -B ./arg_gen/bow_topic.txt -D generic \\\n","  #     --class_label 0 --cond_text '{cond_text}' --grad_length {grad_len} \\\n","  #     --length {length} --gamma 1.0 --num_iterations 5 --num_samples 5 \\\n","  #     --stepsize {stepsize} --kl_scale {kl_scale} --gm_scale 0.99 --colorama \\\n","  #     --sample --discrim_weights /content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_epoch_8.pt \\\n","  #     --discrim_meta /content/drive/MyDrive/PPLM/arg_gen/generic_classifier_head_meta.json --verbosity quiet\"\n","  #)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_8ZFdTf8fg3","executionInfo":{"status":"ok","timestamp":1666863141587,"user_tz":-360,"elapsed":5,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"}},"outputId":"0ff72bc5-c49e-4ae9-c531-ad0ec44b10e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["os.chdir(os.path.join(drive_path, 'PPLM'))\n","print(os.getcwd())"],"metadata":{"id":"926Wb4u46POO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def BOW_writer(aspect):\n","  f = open('/content/drive/MyDrive/PPLM/arg_gen/b1.txt', 'w')\n","  f.write(str(aspect))\n","  return True"],"metadata":{"id":"U-gsp2kF_uwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/PPLM/requirements.txt"],"metadata":{"id":"n44p87Zm6ak_","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1666863258785,"user_tz":-360,"elapsed":116732,"user":{"displayName":"Saiki Sarkar","userId":"03100188859317786815"}},"outputId":"9327c6fb-ffa9-4880-fdb0-446b57e32516"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.7.0\n","  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n","\u001b[K     |████████████████████████████████| 776.7 MB 4.5 kB/s \n","\u001b[?25hCollecting nltk==3.4.5\n","  Downloading nltk-3.4.5.zip (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 52.9 MB/s \n","\u001b[?25hCollecting colorama==0.4.4\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting transformers==3.4.0\n","  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 59.9 MB/s \n","\u001b[?25hCollecting torchtext==0.3.1\n","  Downloading torchtext-0.3.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 1)) (4.1.1)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (21.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (3.17.3)\n","Collecting tokenizers==0.9.2\n","  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 53.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (2022.6.2)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 58.7 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (2022.9.24)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r /content/drive/MyDrive/Shan/PPLM/requirements.txt (line 4)) (1.2.0)\n","Building wheels for collected packages: nltk, sacremoses\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=1888550d89e918cfe21d91eadaec14af2033e7a945600b595ce7e493264f92ba\n","  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fc8b1f834fd18b29bab23fb31431c974e615271da4607fb1cf6ce49766b37a7d\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built nltk sacremoses\n","Installing collected packages: dataclasses, torch, tokenizers, sentencepiece, sacremoses, transformers, torchtext, nltk, colorama\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\u001b[0m\n","Successfully installed colorama-0.4.4 dataclasses-0.6 nltk-3.4.5 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.9.2 torch-1.7.0 torchtext-0.3.1 transformers-3.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","nltk","torch"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCaSXUSzIxBr"},"outputs":[],"source":["i_sentences = [\n","    \"Is Lending Money At Interest Wrong?\"\n","]\n","\n","cond_text = [\n","    \"financial interest\"\n","]\n","\n","\n","idx = 1\n","num_samples = 3\n","for i in tqdm(range(len(cond_text)), ascii=True):\n","  inp = i_sentences[i]\n","  cond = cond_text[i]\n","  aspect = aspects[i]\n","  bow = BOW_writer(aspect)\n","  if bow:\n","     generate_bow(inp, aspect=aspect)\n","     # Will generate num_samples perturbed samples for each input triplet\n","     # (inp, cond, aspect). Saved to ./arg_gen_outputs_{idx}.txt\n","     path_gen = os.path.join(drive_path,'PPLM', f'arg_gen_outputs_{idx}.txt')\n","     print(path_gen)\n","     #with open(path_gen, 'w+') as f:\n","     with open(path_gen,'w+') as f:\n","       f.write(f\"Input: {inp}\\n\")\n","       f.write(f\"Conditional text: {cond}\\n\")\n","       f.write(f\"Aspect: {aspect}\\n\")\n","     run_model(cond, num_samples=num_samples, idx=idx)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.7.9 ('.env_old': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"4d1d6b33449c36b5f62bda22e2b44b95ded49c983a39dd525bb9e9e5c550bcad"}}},"nbformat":4,"nbformat_minor":0}